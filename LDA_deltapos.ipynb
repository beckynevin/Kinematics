{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This LDA code incorporates all of the kinematic predictors,\n",
    "runs statistical analysis (linear + logistic regression) on all predictors (no x-terms),\n",
    "and makes plots.\n",
    "\n",
    "It also runs logistic regression on all of the predictors and x-terms selected by the LDA, which is\n",
    "roughly equivalent to what the LDA would be without priors.\n",
    "\n",
    "This code is useful to determine which terms are necessary out of the following (and all of the x-terms):\n",
    "Delta PA, v_asym, sig_asym, kinemetry resids, lambda_r_e, epsilon, A, A_2, deltapos, deltapos2\n",
    "\n",
    "It eliminates v_asym and A.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Code\n",
      "Beginning\n"
     ]
    }
   ],
   "source": [
    "print('Start Code')\n",
    "'''\n",
    "~~~\n",
    "Separate major merger and minor merger runs\n",
    "~~~\n",
    "'''\n",
    "\n",
    "print('Beginning')\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.signal import argrelextrema\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def dPA_sa(row):\n",
    "    return row['Delta PA']*row['s_asym']\n",
    "def dPA_resids(row):\n",
    "    return row['Delta PA']*row['resids']\n",
    "def dPA_lambdar(row):\n",
    "    return row['Delta PA']*row['lambda_r']\n",
    "def dPA_epsilon(row):\n",
    "    return row['Delta PA']*row['epsilon']\n",
    "def dPA_A2(row):\n",
    "    return row['Delta PA']*row['A_2']\n",
    "def dPA_deltapos(row):\n",
    "    return row['Delta PA']*row['deltapos']\n",
    "def dPA_deltapos2(row):\n",
    "    return row['Delta PA']*row['deltapos2']\n",
    "\n",
    "\n",
    "def sa_resids(row):\n",
    "    return row['s_asym']*row['resids']\n",
    "def sa_lambdar(row):\n",
    "    return row['s_asym']*row['lambda_r']\n",
    "def sa_epsilon(row):\n",
    "    return row['s_asym']*row['epsilon']\n",
    "def sa_A2(row):\n",
    "    return row['s_asym']*row['A_2']\n",
    "def sa_deltapos(row):\n",
    "    return row['s_asym']*row['deltapos']\n",
    "def sa_deltapos2(row):\n",
    "    return row['s_asym']*row['deltapos2']\n",
    "\n",
    "\n",
    "\n",
    "def resids_lambdar(row):\n",
    "    return row['resids']*row['lambda_r']\n",
    "def resids_epsilon(row):\n",
    "    return row['resids']*row['epsilon']\n",
    "def resids_A2(row):\n",
    "    return row['resids']*row['A_2']\n",
    "def resids_deltapos(row):\n",
    "    return row['resids']*row['deltapos']\n",
    "def resids_deltapos2(row):\n",
    "    return row['resids']*row['deltapos2']\n",
    "\n",
    "def lambdar_epsilon(row):\n",
    "    return row['lambda_r']*row['epsilon']\n",
    "def lambdar_A2(row):\n",
    "    return row['lambda_r']*row['A_2']\n",
    "def lambdar_deltapos(row):\n",
    "    return row['lambda_r']*row['deltapos']\n",
    "def lambdar_deltapos2(row):\n",
    "    return row['lambda_r']*row['deltapos2']\n",
    "\n",
    "def epsilon_A2(row):\n",
    "    return row['epsilon']*row['A_2']\n",
    "def epsilon_deltapos(row):\n",
    "    return row['epsilon']*row['deltapos']\n",
    "def epsilon_deltapos2(row):\n",
    "    return row['epsilon']*row['deltapos2']\n",
    "\n",
    "def A2_deltapos(row):\n",
    "    return row['A_2']*row['deltapos']\n",
    "def A2_deltapos2(row):\n",
    "    return row['A_2']*row['deltapos2']\n",
    "\n",
    "def deltapos_deltapos2(row):\n",
    "    return row['deltapos']*row['deltapos']\n",
    "\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.signal import argrelextrema\n",
    "import sklearn.metrics as metrics\n",
    "  \n",
    "\n",
    "def plot_confusion_matrix(cm, target_names, title, cmap=plt.cm.Blues):\n",
    "    print('target names', target_names)\n",
    "    sns.set_style(\"dark\")\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    #plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(target_names))\n",
    "    target_names=['Nonmerger','Merger']\n",
    "    plt.xticks(tick_marks, target_names)#, rotation=45)\n",
    "    plt.yticks(tick_marks, target_names)\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel('True label', size=20)\n",
    "    plt.xlabel('Predicted label', size=20)\n",
    "    \n",
    "    fmt = '.2f' \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "\n",
    "\n",
    "def lda_classify(v, levels, cutoffpoints):\n",
    "    for level, cutoff in zip(reversed(levels), reversed(cutoffpoints)):\n",
    "        if v > cutoff: return level\n",
    "    return levels[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_mean_and_CI(mean, lb, ub, color_mean=None, color_shading=None):\n",
    "        # plot the shaded range of the confidence intervals\n",
    "        plt.fill_between(range(mean.shape[0]), ub, lb,\n",
    "                         color=color_shading, alpha=.5)\n",
    "        # plot the mean on top\n",
    "        plt.plot(mean, color_mean)\n",
    "\n",
    "def group_consecutives(vals, step=1):\n",
    "    \"\"\"Return list of consecutive lists of numbers from vals (number list).\"\"\"\n",
    "    run = []\n",
    "    result = [run]\n",
    "    expect = None\n",
    "    for v in vals:\n",
    "        if (v == expect) or (expect is None):\n",
    "            run.append(v)\n",
    "        else:\n",
    "            run = [v]\n",
    "            result.append(run)\n",
    "        expect = v + step\n",
    "    return result\n",
    "feature_dict = {i:label for i,label in zip(\n",
    "                range(13),\n",
    "                  ('Counter',\n",
    "                  'Image',\n",
    "                  'class label',\n",
    "                  'Myr',\n",
    "                  'Viewpoint',\n",
    "                'Delta PA',\n",
    "                  'v_asym',\n",
    "                  's_asym',\n",
    "                  'resids',\n",
    "                  'lambda_r',\n",
    "                  'epsilon',\n",
    "                  'A',\n",
    "                  'A_2',\n",
    "                  'deltapos',\n",
    "                  'deltapos2'))}\n",
    "\n",
    "def locate_min(a):\n",
    "        smallest = min(a)\n",
    "        return smallest, [index for index, element in enumerate(a) \n",
    "                      if smallest == element]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''''\n",
    "~~~\n",
    "Separate major merger and minor merger runs\n",
    "~~~\n",
    "'''\n",
    "\n",
    "    \n",
    "\n",
    "list_runs=['fg3_m12_center', 'fg1_m13_rebin','fg3_m13_rebin', 'fg3_m15_rebin', 'fg3_m1_10_rebin','major_all_rebin','minor_all_rebin']\n",
    "#list_runs=['fg3_m12_highz']\n",
    "#list_runs=['fg3_m1_10_rebin']#,'fg3_m15']\n",
    "#list_runs=['minor_all']\n",
    "\n",
    "colors=[sns.xkcd_rgb[\"red\"],sns.xkcd_rgb[\"amber\"],sns.xkcd_rgb[\"orange pink\"],sns.xkcd_rgb[\"baby purple\"],sns.xkcd_rgb[\"purple\"],sns.xkcd_rgb[\"reddish orange\"],sns.xkcd_rgb[\"rich purple\"]]\n",
    "names=['q0.5_fg0.3','q0.333_fg0.1','q0.333_fg0.3','q0.2_fg0.3_BT0.2','q0.1_fg0.3_BT0.2','Major Mergers','Minor Mergers']\n",
    "\n",
    "priors_list=[[0.9,0.1],[0.9,0.1],[0.9,0.1],[0.7,0.3],[0.7,0.3],[0.9,0.1],[0.7,0.3]]\n",
    "\n",
    "'''list_runs=['all']\n",
    "colors=[sns.xkcd_rgb[\"aqua blue\"]]\n",
    "names=['All Combined']\n",
    "priors_list=[[0.75,0.25]]'''\n",
    "\n",
    "list_runs=['major_all','minor_all','fg3_m1_10','fg3_m13','fg3_m12','fg3_m15','fg1_m13']\n",
    "#sns.xkcd_rgb[\"aqua\"],sns.xkcd_rgb[\"grass green\"],sns.xkcd_rgb[\"dark pink\"],sns.xkcd_rgb[\"greenish blue\"]\n",
    "colors=[sns.xkcd_rgb[\"dark pink\"],sns.xkcd_rgb[\"greenish blue\"],sns.xkcd_rgb[\"aqua\"],sns.xkcd_rgb[\"orange pink\"],sns.xkcd_rgb[\"red\"],sns.xkcd_rgb[\"grass green\"],sns.xkcd_rgb[\"amber\"]]\n",
    "names=['Major Mergers', 'Minor Mergers','q0.1_fg0.3_BT0.2','q0.333_fg0.3','q0.5_fg0.3','q0.2_fg0.3_BT0.2','q0.333_fg0.1']\n",
    "\n",
    "priors_list=[[0.9,0.1],[0.7,0.3],[0.7,0.3],[0.9,0.1],[0.9,0.1],[0.7,0.3],[0.9,0.1]]\n",
    "#priors_list=[[0.5,0.5],[0.5,0.5],[0.5,0.5],[0.5,0.5],[0.5,0.5],[0.5,0.5],[0.5,0.5]]\n",
    "\n",
    "\n",
    "list_runs=['fg3_m12', 'fg3_m12_notfixbug']\n",
    "#sns.xkcd_rgb[\"aqua\"],sns.xkcd_rgb[\"grass green\"],sns.xkcd_rgb[\"dark pink\"],sns.xkcd_rgb[\"greenish blue\"]\n",
    "colors=[sns.xkcd_rgb[\"red\"], sns.xkcd_rgb[\"grass green\"]]\n",
    "names=['q0.5_fg0.3', 'q0.5_fg0.3']\n",
    "\n",
    "priors_list=[[0.9,0.1],[0.7,0.3]]\n",
    "#priors_list=[[0.5,0.5],[0.5,0.5],[0.5,0.5],[0.5,0.5],[0.5,0.5],[0.5,0.5],[0.5,0.5]]\n",
    "\n",
    "\n",
    "#myr_total [2.7399999999999998, 2.7399999999999998, 9.17, 9.17, 9.17, 9.17, 2.6399999999999997, 2.6399999999999997, 2.1999999999999997, 2.1999999999999997, 3.5200000000000005, 3.5200000000000005, 2.83, 2.83]\n",
    "#LDA_time [2.1800000000000006, 4.31, 3.3099999999999996, 2.1599999999999997, 1.6099999999999999, 3.5200000000000005, 1.7100000000000004]\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "missclass_list=[]\n",
    "\n",
    "\n",
    "LDA_time=[]\n",
    "myr_total=[]\n",
    "\n",
    "'''num_comps_list.append(num_comps)\n",
    "    missclass_list.append(missclass)\n",
    "    min_comps_list.append(min_comps)\n",
    "    min_A_list.append(min_A)'''\n",
    "num_comps_list=[]\n",
    "missclass_list_1=[]\n",
    "missclass_list_1_e=[]\n",
    "min_comps_list=[]\n",
    "min_A_list=[]\n",
    "\n",
    "dPA_means_all=[]\n",
    "va_means_all=[]\n",
    "color_means_all=[]\n",
    "sa_means_all=[]\n",
    "resids_means_all=[]\n",
    "lambdar_means_all=[]\n",
    "epsilon_means_all=[]\n",
    "\n",
    "A2_means_all=[]\n",
    "deltapos_means_all=[]\n",
    "deltapos2_means_all=[]\n",
    "\n",
    "dPA_means_all_non=[]\n",
    "va_means_all_non=[]\n",
    "color_means_all_non=[]\n",
    "sa_means_all_non=[]\n",
    "resids_means_all_non=[]\n",
    "lambdar_means_all_non=[]\n",
    "epsilon_means_all_non=[]\n",
    "\n",
    "A2_means_all_non=[]\n",
    "deltapos_means_all_non=[]\n",
    "deltapos2_means_all_non=[]\n",
    "\n",
    "for i in range(1):#len(list_runs)):\n",
    "    #i=i+5\n",
    "    add_on=list_runs[i]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    run=list_runs[i]\n",
    "\n",
    "    df = pd.io.parsers.read_table(\n",
    "        filepath_or_buffer='LDA_kin_rerecenter_'+str(run)+'.txt',#'_view_all.txt',#filepath_or_buffer='LDA_img_ratio_'+str(run)+'_early_late_all_things.txt',#'_view_all.txt',\n",
    "        header=[0],\n",
    "        sep='\\t'\n",
    "        )\n",
    "    \n",
    "    \n",
    "    #df.columns = [l for i,l in sorted(feature_dict.items())] + ['Shape Asymmetry']\n",
    "\n",
    "    df.dropna(how=\"all\", inplace=True) # to drop the empty line at file-end\n",
    "\n",
    "\n",
    "    for j in range(len(df)):\n",
    "        if df[['Myr']].values[j][0]<0.39:#df[['Myr']].values[i][0]\n",
    "            df.set_value(j,'class label',0)\n",
    "        if add_on[:7]=='fg3_m12' and (df[['Myr']].values[j][0]-2.15) > 0.5:#0.39+0.1:#was 0.39\n",
    "            df.set_value(j,'class label',0)\n",
    "        if add_on[:7]=='fg1_m13' and (df[['Myr']].values[j][0]-2.74) > 0.5:#2.74-2.25+2.74:#was 0.39\n",
    "            df.set_value(j,'class label',0)\n",
    "        if add_on[:7]=='fg3_m13' and (df[['Myr']].values[j][0]-2.59) > 0.5:#2.64+0.5\n",
    "            df.set_value(j,'class label',0)\n",
    "        if add_on[:7]=='fg3_m15' and (df[['Myr']].values[j][0]-3.72) > 0.5:\n",
    "            df.set_value(j,'class label',0)\n",
    "        if add_on[:9]=='fg3_m1_10' and (df[['Myr']].values[j][0]-9.17) > 0.5:\n",
    "            df.set_value(j,'class label',0)\n",
    "        if add_on[:9]=='major_all':\n",
    "            #then sort by image name\n",
    "            if df[['Image']].values[j][0][:7]=='fg3_m12' and (df[['Myr']].values[j][0]-2.15) > 0.5:#q0.5_fg0.3\n",
    "                df.set_value(j,'class label',0)\n",
    "            if df[['Image']].values[j][0][:7]=='fg1_m13' and (df[['Myr']].values[j][0]-2.59) > 0.5:\n",
    "                df.set_value(j,'class label',0)\n",
    "            if df[['Image']].values[j][0][:7]=='fg3_m13' and (df[['Myr']].values[j][0]-2.74) > 0.5:\n",
    "                df.set_value(j,'class label',0)\n",
    "        if add_on[:9]=='minor_all':\n",
    "            #then sort by image name\n",
    "            if df[['Image']].values[j][0][:7]=='fg3_m15' and (df[['Myr']].values[j][0]-3.72) > 0.5:\n",
    "                df.set_value(j,'class label',0)\n",
    "            if df[['Image']].values[j][0][:7]=='fg3_m10' and (df[['Myr']].values[j][0]-9.17) > 0.5:\n",
    "                df.set_value(j,'class label',0)\n",
    "            \n",
    "    '''if add_on[:7]=='minor_all':\n",
    "        new_df=df\n",
    "        file_write=open('LDA_kin_all_combined_minor.txt','w')\n",
    "        file_write.write('Counter'+'\\t'+'Image'+'\\t'+'Merger (0 = no, 1 = yes)'+'\\t'+\n",
    "                'Myr'+'\\t'+'Viewpoint'+'\\t'+'# Bulges'+'\\t'+'Sep'+'\\t'+'Flux Ratio'+'\\t'+'Gini'+'\\t'+'M20'+'\\t'+'C'+'\\t'+'A'+'\\t'+'S'+\n",
    "                        '\\t'+'Sersic n'+'\\t'+'A_s'+'\\n')\n",
    "        counter=0\n",
    "        for j in range(len(new_df)):\n",
    "            file_write.write(str(counter)+'\\t'+str(new_df[['Image']].values[j][0])+'\\t'+\n",
    "                            str(new_df[['class label']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Myr']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Viewpoint']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['# Bulges']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Sep']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Flux Ratio']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Gini']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['M20']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Concentration (C)']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Asymmetry (A)']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Clumpiness (S)']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Sersic N']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Shape Asymmetry']].values[j][0])+'\\n')\n",
    "            counter+=1\n",
    "        \n",
    "        \n",
    "        file_write.close()\n",
    "        \n",
    "    if add_on[:7]=='major_all':\n",
    "        new_df=df\n",
    "        file_write=open('LDA_kin_all_combined_major.txt','w')\n",
    "        file_write.write('Counter'+'\\t'+'Image'+'\\t'+'Merger (0 = no, 1 = yes)'+'\\t'+\n",
    "                'Myr'+'\\t'+'Viewpoint'+'\\t'+'# Bulges'+'\\t'+'Sep'+'\\t'+'Flux Ratio'+'\\t'+'Gini'+'\\t'+'M20'+'\\t'+'C'+'\\t'+'A'+'\\t'+'S'+\n",
    "                        '\\t'+'Sersic n'+'\\t'+'A_s'+'\\n')\n",
    "        counter=0\n",
    "        for j in range(len(new_df)):\n",
    "            file_write.write(str(counter)+'\\t'+str(new_df[['Image']].values[j][0])+'\\t'+\n",
    "                            str(new_df[['class label']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Myr']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Viewpoint']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['# Bulges']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Sep']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Flux Ratio']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Gini']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['M20']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Concentration (C)']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Asymmetry (A)']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Clumpiness (S)']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Sersic N']].values[j][0])+'\\t'+\n",
    "                             str(new_df[['Shape Asymmetry']].values[j][0])+'\\n')\n",
    "            counter+=1\n",
    "        \n",
    "        \n",
    "        file_write.close() '''       \n",
    "\n",
    "    myr=[]\n",
    "    myr_non=[]\n",
    "    for j in range(len(df)):\n",
    "        if df[['class label']].values[j][0]==0.0:\n",
    "            myr_non.append(df[['Myr']].values[j][0])\n",
    "        else:\n",
    "            myr.append(df[['Myr']].values[j][0])\n",
    "\n",
    "    myr_non=sorted(list(set(myr_non)))\n",
    "    myr=sorted(list(set(myr)))\n",
    "\n",
    "\n",
    "\n",
    "    print('myr', myr)\n",
    "    print('myr_non', myr_non)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    df.dropna(inplace=True) # to drop the empty line at file-end\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "     \n",
    "    df['dPA*sa'] = df.apply(dPA_sa,axis=1)\n",
    "    df['dPA*resids'] = df.apply(dPA_resids,axis=1)\n",
    "    df['dPA*lambdar'] = df.apply(dPA_lambdar,axis=1)\n",
    "    df['dPA*epsilon'] = df.apply(dPA_epsilon,axis=1)\n",
    "    df['dPA*A2'] = df.apply(dPA_A2,axis=1)\n",
    "    df['dPA*deltapos'] = df.apply(dPA_deltapos,axis=1)\n",
    "    df['dPA*deltapos2'] = df.apply(dPA_deltapos2,axis=1)\n",
    "    \n",
    "    \n",
    "    df['sa*resids'] = df.apply(sa_resids,axis=1)\n",
    "    df['sa*lambdar'] = df.apply(sa_lambdar,axis=1)\n",
    "    df['sa*epsilon'] = df.apply(sa_epsilon,axis=1)\n",
    "    df['sa*A2'] = df.apply(sa_A2,axis=1)\n",
    "    df['sa*deltapos'] = df.apply(sa_deltapos,axis=1)\n",
    "    df['sa*deltapos2'] = df.apply(sa_deltapos2,axis=1)\n",
    "    \n",
    "    \n",
    "    df['resids*lambdar'] = df.apply(resids_lambdar,axis=1)\n",
    "    df['resids*epsilon'] = df.apply(resids_epsilon,axis=1)\n",
    "    df['resids*A2'] = df.apply(resids_A2,axis=1)\n",
    "    df['resids*deltapos'] = df.apply(resids_deltapos,axis=1)\n",
    "    df['resids*deltapos2'] = df.apply(resids_deltapos2,axis=1)\n",
    "    \n",
    "    \n",
    "    df['lambdar*epsilon'] = df.apply(lambdar_epsilon,axis=1)\n",
    "    df['lambdar*A2'] = df.apply(lambdar_A2,axis=1)\n",
    "    df['lambdar*deltapos'] = df.apply(lambdar_deltapos,axis=1)\n",
    "    df['lambdar*deltapos2'] = df.apply(lambdar_deltapos2,axis=1)\n",
    "    \n",
    "    \n",
    "    df['epsilon*A2'] = df.apply(epsilon_A2,axis=1)\n",
    "    df['epsilon*deltapos'] = df.apply(epsilon_deltapos,axis=1)\n",
    "    df['epsilon*deltapos2'] = df.apply(epsilon_deltapos2,axis=1)\n",
    "    \n",
    "    df['A2*deltapos'] = df.apply(A2_deltapos,axis=1)\n",
    "    df['A2*deltapos2'] = df.apply(A2_deltapos2,axis=1)\n",
    "    \n",
    "    df['deltapos*deltapos'] = df.apply(deltapos_deltapos2,axis=1)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    #print(df)\n",
    "    \n",
    "    '''First, do a bunch of diagnostic plotting :)'''\n",
    "    X= df[['Delta PA','v_asym','s_asym', 'resids','lambda_r', 'epsilon','A_2','deltapos','deltapos2']].values\n",
    "    \n",
    "    \n",
    "    dPA = {key:[] for key in myr}\n",
    "    va = {key:[] for key in myr}\n",
    "    sa = {key:[] for key in myr}\n",
    "    resids = {key:[] for key in myr}\n",
    "    lambdar = {key:[] for key in myr}\n",
    "    epsilon = {key:[] for key in myr}\n",
    "    A2 = {key:[] for key in myr}\n",
    "    deltapos = {key:[] for key in myr}\n",
    "    deltapos2 = {key:[] for key in myr}\n",
    "    \n",
    "    \n",
    "    \n",
    "    dPA_std = {key:[] for key in myr}\n",
    "    va_std = {key:[] for key in myr}\n",
    "    sa_std = {key:[] for key in myr}\n",
    "    resids_std = {key:[] for key in myr}\n",
    "    lambdar_std = {key:[] for key in myr}\n",
    "    epsilon_std = {key:[] for key in myr}\n",
    "    A2_std = {key:[] for key in myr}\n",
    "    deltapos_std = {key:[] for key in myr}\n",
    "    deltapos2_std = {key:[] for key in myr}\n",
    "    \n",
    "\n",
    "    dPA_non = {key:[] for key in myr_non}\n",
    "    va_non = {key:[] for key in myr_non}\n",
    "    sa_non = {key:[] for key in myr_non}\n",
    "    resids_non = {key:[] for key in myr_non}\n",
    "    lambdar_non = {key:[] for key in myr_non}\n",
    "    epsilon_non = {key:[] for key in myr_non}\n",
    "    A2_non = {key:[] for key in myr_non}\n",
    "    deltapos_non = {key:[] for key in myr_non}\n",
    "    deltapos2_non = {key:[] for key in myr_non}\n",
    "\n",
    "    \n",
    "        \n",
    "    color_name_non = {key:[] for key in myr_non}\n",
    "    color_name = {key:[] for key in myr}\n",
    "    \n",
    "    marker_name_non = {key:[] for key in myr_non}\n",
    "    marker_name = {key:[] for key in myr}\n",
    "\n",
    "\n",
    "    epsilon_all=[]\n",
    "    epsilon_all_non=[]\n",
    "    vasym_all=[]\n",
    "    vasym_all_non=[]\n",
    "    \n",
    "    for l in range(len(df)):\n",
    "        if df[['class label']].values[l]==0:\n",
    "            \n",
    "            epsilon_all_non.append(X[l,5])\n",
    "            vasym_all_non.append(X[l,1])\n",
    "            \n",
    "            dPA_non[df[['Myr']].values[l][0]].append(X[l,0])\n",
    "            va_non[df[['Myr']].values[l][0]].append(X[l,1])\n",
    "            sa_non[df[['Myr']].values[l][0]].append(X[l,2])\n",
    "            resids_non[df[['Myr']].values[l][0]].append(X[l,3])\n",
    "            lambdar_non[df[['Myr']].values[l][0]].append(X[l,4])\n",
    "            epsilon_non[df[['Myr']].values[l][0]].append(X[l,5])\n",
    "            A2_non[df[['Myr']].values[l][0]].append(X[l,6])\n",
    "            deltapos_non[df[['Myr']].values[l][0]].append(X[l,7])\n",
    "            deltapos2_non[df[['Myr']].values[l][0]].append(X[l,8])\n",
    "            \n",
    "            \n",
    "            #color_non[df[['Myr']].values[l][0]].append(df[['']])\n",
    "            #print('concentration here', df[['Concentration (C)']].values[i][0])\n",
    "        else:\n",
    "            epsilon_all.append(X[l,5])\n",
    "            vasym_all.append(X[l,1])\n",
    "            \n",
    "            dPA[df[['Myr']].values[l][0]].append(X[l,0])\n",
    "            va[df[['Myr']].values[l][0]].append(X[l,1])\n",
    "            sa[df[['Myr']].values[l][0]].append(X[l,2])\n",
    "            resids[df[['Myr']].values[l][0]].append(X[l,3])\n",
    "            lambdar[df[['Myr']].values[l][0]].append(X[l,4])\n",
    "            epsilon[df[['Myr']].values[l][0]].append(X[l,5])\n",
    "            A2[df[['Myr']].values[l][0]].append(X[l,6])\n",
    "            deltapos[df[['Myr']].values[l][0]].append(X[l,7])\n",
    "            deltapos2[df[['Myr']].values[l][0]].append(X[l,8])\n",
    "            \n",
    "            \n",
    "    \n",
    "    '''First show if anything trends more with inclination than with merger vs nonmerger'''\n",
    "    \n",
    "    \n",
    "    plt.clf()\n",
    "    '''epsilon_all_non.append(X[l,5])\n",
    "            vasym_all_non.append(X[l,1])'''\n",
    "    plt.scatter(epsilon_all_non, vasym_all_non, color='blue', label='Nonmergers')\n",
    "    plt.scatter(epsilon_all, vasym_all, color='red', label='Mergers')\n",
    "    plt.xlabel('$\\epsilon$')\n",
    "    plt.ylabel('$v_{\\mathrm{asym}}$')\n",
    "    plt.xlim([0,1])\n",
    "    plt.legend()\n",
    "    plt.savefig('epsilon_scatter_'+str(run)+'.pdf')\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''Check for some statistical assumptions'''\n",
    "    '''Testing for normality with KS'''\n",
    "    import scipy\n",
    "    \n",
    "    df_nonmerg = df[df['class label'] == 0]\n",
    "    df_merg = df[df['class label'] == 1]\n",
    "    \n",
    "    IVS=[\"Delta PA\",\"s_asym\",\"resids\",\"lambda_r\",\"epsilon\",\"A_2\",\"deltapos\",\"deltapos2\"]\n",
    "    for k in range(len(IVS)):\n",
    "        print('IV', IVS[k])\n",
    "        ks_results = scipy.stats.kstest(df_merg.filter([IVS[k]]), cdf='norm')\n",
    "        shapiro_results = scipy.stats.shapiro(df_merg.filter([IVS[k]]))\n",
    "        #print('ks_results', ks_results)\n",
    "        print('SW results', shapiro_results)\n",
    "        shapiro_results = scipy.stats.shapiro(df_nonmerg.filter([IVS[k]]))\n",
    "        #print('ks_results', ks_results)\n",
    "        print('SW results', shapiro_results)\n",
    "        \n",
    "    '''Test for non-homogeneity of variance-covariance matrices'''\n",
    "    sample1=df_nonmerg.loc[:,\"Delta PA\":\"deltapos2\"]\n",
    "    #df_nonmerg[\"Gini\",\"M20\",\"Concentration (C)\",\"Asymmetry (A)\",\"Clumpiness (S)\",\"Sersic N\",\"Shape Asymmetry\"]\n",
    "    sample2=df_merg.loc[:,\"Delta PA\":\"deltapos2\"]\n",
    "    #df_merg[\"Gini\",\"M20\",\"Concentration (C)\",\"Asymmetry (A)\",\"Clumpiness (S)\",\"Sersic N\",\"Shape Asymmetry\"]\n",
    "    print('shape sample1', np.shape(sample1), 'shape sample2', np.shape(sample2))\n",
    "    \n",
    "    \n",
    "    plt.clf()\n",
    "    from sci_analysis import analyze\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''Now I would love to run some OLS to find R^2 values etc'''\n",
    "    from sklearn import datasets, linear_model, feature_selection\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    y = df['class label'].values\n",
    "    \n",
    "    \n",
    "    X= df[['Delta PA','v_asym','s_asym', 'resids','lambda_r', 'epsilon','A','A_2','deltapos','deltapos2']].values\n",
    "    #std_scale = preprocessing.StandardScaler().fit(X)\n",
    "    #X = std_scale.transform(X)\n",
    "\n",
    "    enc = LabelEncoder()\n",
    "    label_encoder = enc.fit(y)\n",
    "    y = label_encoder.transform(y) + 1\n",
    "    label_dict = {1: 'Nonmerger', 2: 'Merger'}\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "    #print('len', len(X_train), len(X_test), len(y_train), len(y_test))\n",
    "\n",
    "    # Create linear regression object\n",
    "    regr = linear_model.LinearRegression()\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    regr.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions using the testing set\n",
    "    y_pred = regr.predict(X_test)\n",
    "\n",
    "    # The coefficients\n",
    "    print('F test',feature_selection.f_regression(X_train, y_train))\n",
    "    print('Coefficients: \\n', regr.coef_)\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\"\n",
    "          % mean_squared_error(y_test, y_pred))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % r2_score(y_test, y_pred))\n",
    "    \n",
    "    '''Now time to compute R^2 values for the entire freaking thing'''\n",
    "    for k in range(len(IVS)):\n",
    "        for j in range(len(IVS)):\n",
    "            print('Regressing these IVs', IVS[k], IVS[j])\n",
    "            regr.fit(df.filter([IVS[k]]), df.filter([IVS[j]]))\n",
    "            y_pred = regr.predict(df.filter([IVS[k]]))\n",
    "            print('Variance score: %.2f' % r2_score(df.filter([IVS[j]]), y_pred))\n",
    "    \n",
    "    # Note the difference in argument order\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn import datasets, linear_model\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    import statsmodels.api as sm\n",
    "    from scipy import stats\n",
    "\n",
    "    X2 = sm.add_constant(X)\n",
    "    est = sm.OLS(y, X2)\n",
    "    est2 = est.fit()\n",
    "    print(est2.summary())\n",
    "    \n",
    "    \n",
    "    \n",
    "    #X= df[['Delta PA','s_asym', 'resids','lambda_r', 'epsilon','A_2']].values\n",
    "    \n",
    "    y = df['class label'].values\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    enc = LabelEncoder()\n",
    "    label_encoder = enc.fit(y)\n",
    "    y = label_encoder.transform(y)\n",
    "    print('y', y)\n",
    "    '''try:\n",
    "        logistic = sm.Logit(y, X).fit()\n",
    "        print(logistic.summary())\n",
    "    except LinAlgError:\n",
    "        print('cant do logit here')'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''Calculate correlation'''\n",
    "    plt.clf()\n",
    "    import seaborn as sns\n",
    "    #X = data.loc[:, \"V2\":]\n",
    "    corrmat = df.loc[:,\"Delta PA\":\"deltapos2\"].corr()\n",
    "    '''sns.heatmap(corrmat, vmax=1., square=False).xaxis.tick_top()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('corr_mat_'+str(run)+'.pdf')'''\n",
    "    \n",
    "    print('corrmat',corrmat)\n",
    "    \n",
    "    '''plt.clf()\n",
    "    hinton(corrmat)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('hinton_corr_mat_'+str(run)+'.pdf')'''\n",
    "    \n",
    "    \n",
    "    print('most highly corr',mosthighlycorrelated(df.loc[:,\"Delta PA\":\"deltapos2\"], 10))\n",
    "    \n",
    "    \n",
    "    \n",
    "    dPA_list_all_non=[]\n",
    "    va_list_all_non=[]\n",
    "    sa_list_all_non=[]\n",
    "    resids_list_all_non=[]\n",
    "    lambdar_list_all_non=[]\n",
    "    epsilon_list_all_non=[]\n",
    "    A2_list_all_non=[]\n",
    "    deltapos_list_all_non=[]\n",
    "    deltapos2_list_all_non=[]\n",
    "    \n",
    "    dPA_list_all_non_std=[]\n",
    "    va_list_all_non_std=[]\n",
    "    sa_list_all_non_std=[]\n",
    "    resids_list_all_non_std=[]\n",
    "    lambdar_list_all_non_std=[]\n",
    "    epsilon_list_all_non_std=[]\n",
    "    A2_list_all_non_std=[]\n",
    "    deltapos_list_all_non_std=[]\n",
    "    deltapos2_list_all_non_std=[]\n",
    "    \n",
    "    color_list_all_non=[]\n",
    "    color_list_all_non_std=[]\n",
    "    marker_list_all_non=[]\n",
    "    marker_list_all_non_std=[]\n",
    "    \n",
    "\n",
    "    dPA_list_all=[]\n",
    "    va_list_all=[]\n",
    "    sa_list_all=[]\n",
    "    resids_list_all=[]\n",
    "    lambdar_list_all=[]\n",
    "    epsilon_list_all=[]\n",
    "    A2_list_all=[]\n",
    "    deltapos_list_all=[]\n",
    "    deltapos2_list_all=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    dPA_list_all_std=[]\n",
    "    va_list_all_std=[]\n",
    "    sa_list_all_std=[]\n",
    "    resids_list_all_std=[]\n",
    "    lambdar_list_all_std=[]\n",
    "    epsilon_list_all_std=[]\n",
    "    A2_list_all_std=[]\n",
    "    deltapos_list_all_std=[]\n",
    "    deltapos2_list_all_std=[]\n",
    "    \n",
    "    \n",
    "    color_list_all=[]\n",
    "    color_list_all_std=[]\n",
    "    marker_list_all=[]\n",
    "    marker_list_all_std=[]\n",
    "    \n",
    "    \n",
    "    dPA_means_non=[]\n",
    "    va_means_non=[]\n",
    "    sa_means_non=[]\n",
    "    resids_means_non=[]\n",
    "    lambdar_means_non=[]\n",
    "    epsilon_means_non=[]\n",
    "    A2_means_non=[]\n",
    "    deltapos_means_non=[]\n",
    "    deltapos2_means_non=[]\n",
    "    color_means_non=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    counter_overall_non=0\n",
    "    \n",
    "    for k in range(len(dPA_non)):\n",
    "        for z in range(len(dPA_non[myr_non[k]])):\n",
    "            counter_overall_non+=1\n",
    "            dPA_list_all_non.append((dPA_non[myr_non[k]][z]))\n",
    "            va_list_all_non.append((va_non[myr_non[k]][z]))\n",
    "            sa_list_all_non.append((sa_non[myr_non[k]][z]))\n",
    "            \n",
    "            resids_list_all_non.append((resids_non[myr_non[k]][z]))\n",
    "            \n",
    "            lambdar_list_all_non.append((lambdar_non[myr_non[k]][z]))\n",
    "            epsilon_list_all_non.append((epsilon_non[myr_non[k]][z]))\n",
    "            A2_list_all_non.append((A2_non[myr_non[k]][z]))\n",
    "            deltapos_list_all_non.append((deltapos_non[myr_non[k]][z]))\n",
    "            deltapos2_list_all_non.append((deltapos2_non[myr_non[k]][z]))\n",
    "            '''if run[:9]=='major_all' or run[:9]=='minor_all':\n",
    "                color_list_all_non.append(color_name_non[myr_non[k]][z])\n",
    "                marker_list_all_non.append(marker_name_non[myr_non[k]][z])\n",
    "                \n",
    "                \n",
    "            else:'''\n",
    "            color_list_all_non.append(myr_non[k]/1)\n",
    "            marker_list_all_non.append(myr_non[k]/1)\n",
    "            \n",
    "            \n",
    "            \n",
    "        dPA_means_non.append(np.mean(dPA_non[myr_non[k]][:]))\n",
    "        va_means_non.append(np.mean(va_non[myr_non[k]][:]))\n",
    "        sa_means_non.append(np.mean(sa_non[myr_non[k]][:]))\n",
    "        resids_means_non.append(np.mean(resids_non[myr_non[k]][:]))\n",
    "        \n",
    "        lambdar_means_non.append(np.mean(lambdar_non[myr_non[k]][:]))\n",
    "        epsilon_means_non.append(np.mean(epsilon_non[myr_non[k]][:]))\n",
    "        A2_means_non.append(np.mean(A2_non[myr_non[k]][:]))\n",
    "        deltapos_means_non.append(np.mean(deltapos_non[myr_non[k]][:]))\n",
    "        deltapos2_means_non.append(np.mean(deltapos2_non[myr_non[k]][:]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        color_means_non.append((myr_non[k]/1))\n",
    "        \n",
    "    dPA_means=[]\n",
    "    va_means=[]\n",
    "    sa_means=[]\n",
    "    resids_means=[]\n",
    "    lambdar_means=[]\n",
    "    epsilon_means=[]\n",
    "    A2_means=[]\n",
    "    deltapos_means=[]\n",
    "    deltapos2_means=[]\n",
    "    color_means=[]\n",
    "    \n",
    "    counter_overall=0\n",
    "    \n",
    "    for k in range(len(dPA)):\n",
    "        for z in range(len(dPA[myr[k]])):\n",
    "            counter_overall+=1\n",
    "            dPA_list_all.append((dPA[myr[k]][z]))\n",
    "            va_list_all.append((va[myr[k]][z]))\n",
    "            sa_list_all.append((sa[myr[k]][z]))\n",
    "            lambdar_list_all.append((lambdar[myr[k]][z]))\n",
    "            epsilon_list_all.append((epsilon[myr[k]][z]))\n",
    "            A2_list_all.append((A2[myr[k]][z]))\n",
    "            deltapos_list_all.append((deltapos[myr[k]][z]))\n",
    "            deltapos2_list_all.append((deltapos2[myr[k]][z]))\n",
    "            '''if run[:9]=='major_all' or run[:9]=='minor_all':\n",
    "                color_list_all.append(color_name[myr[k]][z])\n",
    "                marker_list_all.append(marker_name[myr[k]][z])\n",
    "                \n",
    "            else:'''\n",
    "            color_list_all.append((myr[k]))\n",
    "            marker_list_all.append((myr[k]))#/((t_p-t_e)/2+t_e))\n",
    "            \n",
    "        dPA_means.append(np.mean(dPA[myr[k]][:]))\n",
    "        va_means.append(np.mean(va[myr[k]][:]))\n",
    "        sa_means.append(np.mean(sa[myr[k]][:]))\n",
    "        resids_means.append(np.mean(resids[myr[k]][:]))\n",
    "        lambdar_means.append(np.mean(lambdar[myr[k]][:]))\n",
    "        epsilon_means.append(np.mean(epsilon[myr[k]][:]))\n",
    "        A2_means.append(np.mean(A2[myr[k]][:]))\n",
    "        deltapos_means.append(np.mean(deltapos[myr[k]][:]))\n",
    "        deltapos2_means.append(np.mean(deltapos2[myr[k]][:]))\n",
    "        color_means.append((myr[k]/1))\n",
    "        \n",
    "        \n",
    "    dPA_means_all.append(dPA_means)\n",
    "    va_means_all.append(va_means)\n",
    "    sa_means_all.append(sa_means)\n",
    "    resids_means_all.append(resids_means)\n",
    "    lambdar_means_all.append(lambdar_means)\n",
    "    epsilon_means_all.append(epsilon_means)\n",
    "    A2_means_all.append(A2_means)\n",
    "    deltapos_means_all.append(deltapos_means)\n",
    "    deltapos2_means_all.append(deltapos2_means)\n",
    "    color_means_all.append(color_means)\n",
    "    \n",
    "    \n",
    "    dPA_means_all_non.append(dPA_means_non)\n",
    "    va_means_all_non.append(va_means_non)\n",
    "    sa_means_all_non.append(sa_means_non)\n",
    "    resids_means_all_non.append(resids_means_non)\n",
    "    lambdar_means_all_non.append(lambdar_means_non)\n",
    "    epsilon_means_all_non.append(epsilon_means_non)\n",
    "    A2_means_all_non.append(A2_means_non)\n",
    "    deltapos_means_all_non.append(deltapos_means_non)\n",
    "    deltapos2_means_all_non.append(deltapos2_means_non)\n",
    "    color_means_all_non.append(color_means_non)\n",
    "    \n",
    "    \n",
    "    print('~~~~~means~~~~')\n",
    "    print('means dPA non', np.mean(dPA_means_non), np.std(dPA_means_non))\n",
    "    print('means dPA', np.mean(dPA_means), np.std(dPA_means))\n",
    "    print('~~~~~means~~~~')\n",
    "    print('means va non', np.mean(va_means_non), np.std(va_means_non))\n",
    "    print('means va', np.mean(va_means), np.std(va_means))\n",
    "    print('~~~~~~~~~')\n",
    "    print('~~~~~means~~~~')\n",
    "    print('means sa non', np.mean(sa_means_non), np.std(sa_means_non))\n",
    "    print('means sa', np.mean(sa_means), np.std(sa_means))\n",
    "    print('~~~~~~~~~')\n",
    "    print('~~~~~means~~~~')\n",
    "    print('means resids non', np.mean(resids_means_non), np.std(resids_means_non))\n",
    "    print('means resids', np.mean(resids_means), np.std(resids_means))\n",
    "    print('~~~~~~~~~')\n",
    "    print('~~~~~means~~~~')\n",
    "    print('means lambdar non', np.mean(lambdar_means_non), np.std(lambdar_means_non))\n",
    "    print('means lambdar', np.mean(lambdar_means), np.std(lambdar_means))\n",
    "    print('~~~~~~~~~')\n",
    "    print('~~~~~means~~~~')\n",
    "    print('means epsilon non', np.mean(epsilon_means_non), np.std(epsilon_means_non))\n",
    "    print('means epsilon', np.mean(epsilon_means), np.std(epsilon_means))\n",
    "    print('~~~~~~~~~')\n",
    "    print('~~~~~means~~~~')\n",
    "    print('means A2 non', np.mean(A2_means_non), np.std(A2_means_non))\n",
    "    print('means A2', np.mean(A2_means), np.std(A2_means))\n",
    "    print('~~~~~~~~~')\n",
    "    print('~~~~~means~~~~')\n",
    "    print('means deltapos non', np.mean(deltapos_means_non), np.std(deltapos_means_non))\n",
    "    print('means deltapos', np.mean(deltapos_means), np.std(deltapos_means))\n",
    "    print('~~~~~~~~~')\n",
    "    print('~~~~~means~~~~')\n",
    "    print('means deltapos2 non', np.mean(deltapos2_means_non), np.std(deltapos2_means_non))\n",
    "    print('means deltapos2', np.mean(deltapos2_means), np.std(deltapos2_means))\n",
    "    print('~~~~~~~~~')\n",
    "    print(run)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "\n",
    "    '''The two plots were definitely going to want to make are the v_asy, sigma_asym plot from Shapiro\n",
    "    and the lambda_r, epsilon plot from Emsellem2007,2011,Cappellari'''\n",
    "    dashed_line_x=np.linspace(0.01,0.5,100)\n",
    "    #the equation is sqrt(y**2+x**2) = 0.5\n",
    "    #y = np.sqrt(0.5**2 - x**2)\n",
    "    dashed_line_y=[np.sqrt(0.5**2-x**2) for x in dashed_line_x]\n",
    "    dotted_line_x=np.linspace(0.01,0.15,100)\n",
    "    dotted_line_y=[np.sqrt(0.15**2-x**2) for x in dotted_line_x]\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''Make the beautiful list of colors'''\n",
    "    # These are the \"Tableau 20\" colors as RGB.    \n",
    "    tableau20 = [(31, 119, 180), (174, 199, 232), (255, 127, 14), (255, 187, 120),    \n",
    "                 (44, 160, 44), (152, 223, 138), (214, 39, 40), (255, 152, 150),    \n",
    "                 (148, 103, 189), (197, 176, 213), (140, 86, 75), (196, 156, 148),    \n",
    "                 (227, 119, 194), (247, 182, 210), (127, 127, 127), (199, 199, 199),    \n",
    "                 (188, 189, 34), (219, 219, 141), (23, 190, 207), (158, 218, 229)]    \n",
    "\n",
    "    # Scale the RGB values to the [0, 1] range, which is the format matplotlib accepts.    \n",
    "    for k in range(len(tableau20)):    \n",
    "        r, g, b = tableau20[k]    \n",
    "        tableau20[k] = (r / 255., g / 255., b / 255.)    \n",
    "\n",
    "\n",
    "    \n",
    "    import seaborn\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    \n",
    "    plt.clf()\n",
    "    fig=plt.figure()\n",
    "    ax1=fig.add_subplot(111)\n",
    "    \n",
    "    \n",
    "    im1=ax1.scatter(deltapos2_means, deltapos_means, c=color_means, cmap='Reds', s=35,edgecolors= \"red\")\n",
    "    cb=plt.colorbar(im1,orientation='vertical')\n",
    "    ax1.annotate('Time [Gyr]', xy=(1.08,1.05), xycoords='axes fraction', size=20)\n",
    "    cb.ax.tick_params(labelsize=20)\n",
    "    \n",
    "    '''for j,k,l in zip(sa_means,va_means,color_means):\n",
    "        ax1.annotate('%s' %l, xy=(j,k), xytext=(j,k))'''\n",
    "        \n",
    "    im2=ax1.scatter(deltapos2_means_non, deltapos_means_non, c=color_means_non, cmap='Blues', s=35,edgecolors= \"blue\")\n",
    "    cb_1= plt.colorbar(im2, orientation='vertical')\n",
    "    cb_1.ax.tick_params(labelsize=20)\n",
    "    \n",
    "    \n",
    "    '''for j,k,l in zip(sa_means_non,va_means_non,color_means_non):\n",
    "        ax1.annotate('%s' %l, xy=(j,k), xytext=(j,k))'''\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    #ax1.set_xlim([0.01,10])#[max(m20_means),min(m20_means)])\n",
    "    #ax1.set_ylim([0.01,10])#ax1.set_ylim([0.3,0.8])\n",
    "    ax1.set_xlabel('$\\Delta$x$_{\\mathrm{vel}}$', size=20)\n",
    "    ax1.set_ylabel(r'$\\Delta$x', size=20)\n",
    "    #ax1.set_aspect(abs(max(m20_means)-min(m20_means))/abs(min(gini_means)-max(gini_means)))\n",
    "    #ax1.set_yscale('log')\n",
    "    #ax1.set_xscale('log')\n",
    "\n",
    "\n",
    "    #ax1.set_title('Mergers', loc='right')\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "    #plt.tight_layout()\n",
    "    \n",
    "    if add_on[:9]=='major_all':\n",
    "        plt.annotate('Major Mergers', xy=(0.02,1.05), xycoords='axes fraction', size=20)\n",
    "    if add_on[:9]=='minor_all':\n",
    "        plt.annotate('Minor Mergers', xy=(0.02,1.05), xycoords='axes fraction', size=20)\n",
    "    \n",
    "    #plt.tight_layout()\n",
    "    plt.savefig('Figures/time_evo_deltapos2_deltapos_'+str(run)+'.pdf', bbox_inches=\"tight\")\n",
    "    \n",
    "    \n",
    "    plt.clf()\n",
    "    fig=plt.figure()\n",
    "    ax1=fig.add_subplot(111)\n",
    "    \n",
    "    \n",
    "    im1=ax1.scatter(resids_means, deltapos_means, c=color_means, cmap='Reds', s=35,edgecolors= \"red\")\n",
    "    cb=plt.colorbar(im1,orientation='vertical')\n",
    "    ax1.annotate('Time [Gyr]', xy=(1.08,1.05), xycoords='axes fraction', size=20)\n",
    "    cb.ax.tick_params(labelsize=20)\n",
    "    \n",
    "    '''for j,k,l in zip(sa_means,va_means,color_means):\n",
    "        ax1.annotate('%s' %l, xy=(j,k), xytext=(j,k))'''\n",
    "        \n",
    "    im2=ax1.scatter(resids_means_non, deltapos_means_non, c=color_means_non, cmap='Blues', s=35,edgecolors= \"blue\")\n",
    "    cb_1= plt.colorbar(im2, orientation='vertical')\n",
    "    cb_1.ax.tick_params(labelsize=20)\n",
    "    \n",
    "    \n",
    "    '''for j,k,l in zip(sa_means_non,va_means_non,color_means_non):\n",
    "        ax1.annotate('%s' %l, xy=(j,k), xytext=(j,k))'''\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    #ax1.set_xlim([0.01,10])#[max(m20_means),min(m20_means)])\n",
    "    #ax1.set_ylim([0.01,10])#ax1.set_ylim([0.3,0.8])\n",
    "    ax1.set_xlabel('resids', size=20)\n",
    "    ax1.set_ylabel(r'$\\Delta$x', size=20)\n",
    "    #ax1.set_aspect(abs(max(m20_means)-min(m20_means))/abs(min(gini_means)-max(gini_means)))\n",
    "    #ax1.set_yscale('log')\n",
    "    #ax1.set_xscale('log')\n",
    "\n",
    "\n",
    "    #ax1.set_titlte('Mergers', loc='right')\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "    #plt.tight_layout()\n",
    "    \n",
    "    if add_on[:9]=='major_all':\n",
    "        plt.annotate('Major Mergers', xy=(0.02,1.05), xycoords='axes fraction', size=20)\n",
    "    if add_on[:9]=='minor_all':\n",
    "        plt.annotate('Minor Mergers', xy=(0.02,1.05), xycoords='axes fraction', size=20)\n",
    "    \n",
    "    #plt.tight_layout()\n",
    "    plt.savefig('Figures/time_evo_resids_deltapos_'+str(run)+'.pdf', bbox_inches=\"tight\")\n",
    "    \n",
    "    \n",
    "    plt.clf()\n",
    "    fig=plt.figure()\n",
    "    ax1=fig.add_subplot(111)\n",
    "    ax1.plot(dashed_line_x, dashed_line_y, ls='--', color='black')\n",
    "    ax1.plot(dotted_line_x, dotted_line_y, ls=':', color='black')\n",
    "\n",
    "\n",
    "\n",
    "        #ax1.scatter(m20[myr[k]],gini[myr[k]],color=tableau20[k], label=myr[k])\n",
    "    #print(np.shape(m20),np.shape(myr))\n",
    "    #ax1.scatter(m20,gini,c=myr)\n",
    "    #plt.colorbar(im1)\n",
    "    \n",
    "    \n",
    "    im1=ax1.scatter(sa_means, va_means, c=color_means, cmap='Reds', s=35,edgecolors= \"red\")\n",
    "    cb=plt.colorbar(im1,orientation='vertical')\n",
    "    ax1.annotate('Time [Gyr]', xy=(1.08,1.05), xycoords='axes fraction', size=20)\n",
    "    cb.ax.tick_params(labelsize=20)\n",
    "    \n",
    "    '''for j,k,l in zip(sa_means,va_means,color_means):\n",
    "        ax1.annotate('%s' %l, xy=(j,k), xytext=(j,k))'''\n",
    "        \n",
    "    im2=ax1.scatter(sa_means_non, va_means_non, c=color_means_non, cmap='Blues', s=35,edgecolors= \"blue\")\n",
    "    cb_1= plt.colorbar(im2, orientation='vertical')\n",
    "    cb_1.ax.tick_params(labelsize=20)\n",
    "    \n",
    "    \n",
    "    '''for j,k,l in zip(sa_means_non,va_means_non,color_means_non):\n",
    "        ax1.annotate('%s' %l, xy=(j,k), xytext=(j,k))'''\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    ax1.set_xlim([0.01,10])#[max(m20_means),min(m20_means)])\n",
    "    ax1.set_ylim([0.01,10])#ax1.set_ylim([0.3,0.8])\n",
    "    ax1.set_xlabel('$\\sigma_{\\mathrm{asym}}$', size=20)\n",
    "    ax1.set_ylabel(r'v$_{\\mathrm{asym}}$', size=20)\n",
    "    #ax1.set_aspect(abs(max(m20_means)-min(m20_means))/abs(min(gini_means)-max(gini_means)))\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_xscale('log')\n",
    "\n",
    "\n",
    "    #ax1.set_title('Mergers', loc='right')\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "    #plt.tight_layout()\n",
    "    \n",
    "    if add_on[:9]=='major_all':\n",
    "        plt.annotate('Major Mergers', xy=(0.02,1.05), xycoords='axes fraction', size=20)\n",
    "    if add_on[:9]=='minor_all':\n",
    "        plt.annotate('Minor Mergers', xy=(0.02,1.05), xycoords='axes fraction', size=20)\n",
    "    \n",
    "    #plt.tight_layout()\n",
    "    plt.savefig('Figures/time_evo_va_sa_'+str(run)+'.pdf', bbox_inches=\"tight\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''The two plots were definitely going to want to make are the v_asy, sigma_asym plot from Shapiro\n",
    "    and the lambda_r, epsilon plot from Emsellem2007,2011,Cappellari'''\n",
    "    dashed_line_x=np.linspace(0.0,0.4,100)\n",
    "    #the equation is sqrt(y**2+x**2) = 0.5\n",
    "    #y = np.sqrt(0.5**2 - x**2)\n",
    "    dashed_line_y=[0.08+x/4 for x in dashed_line_x]\n",
    "    \n",
    "    dashed_line_x_2 = 0.4*np.ones(100)\n",
    "    dashed_line_y_2 = np.linspace(0,0.18,100)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "      \n",
    "\n",
    "\n",
    "    \n",
    "    import seaborn\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    \n",
    "    plt.clf()\n",
    "    fig=plt.figure()\n",
    "    ax1=fig.add_subplot(111)\n",
    "    ax1.plot(dashed_line_x, dashed_line_y, ls='--', color='black')\n",
    "    ax1.plot(dashed_line_x_2, dashed_line_y_2, ls='--', color='black')\n",
    "\n",
    "\n",
    "\n",
    "        #ax1.scatter(m20[myr[k]],gini[myr[k]],color=tableau20[k], label=myr[k])\n",
    "    #print(np.shape(m20),np.shape(myr))\n",
    "    #ax1.scatter(m20,gini,c=myr)\n",
    "    #plt.colorbar(im1)\n",
    "    \n",
    "    \n",
    "    im1=ax1.scatter(epsilon_means,lambdar_means,  s=35,c=color_means, cmap='Reds',edgecolors= \"red\")\n",
    "    cb=plt.colorbar(im1,orientation='vertical')\n",
    "    ax1.annotate('Time [Gyr]', xy=(1.08,1.05), xycoords='axes fraction', size=20)\n",
    "    cb.ax.tick_params(labelsize=20)\n",
    "    \n",
    "\n",
    "    \n",
    "    '''for j,k,l in zip(epsilon_means,lambdar_means,color_means):\n",
    "        ax1.annotate('%s' %l, xy=(j,k), xytext=(j,k))'''\n",
    "        \n",
    "    \n",
    "    im2=ax1.scatter(epsilon_means_non,lambdar_means_non,  s=35,c=color_means_non, cmap='Blues',edgecolors= \"blue\")\n",
    "    cb2=plt.colorbar(im2,orientation='vertical')\n",
    "    cb2.ax.tick_params(labelsize=20)\n",
    "    \n",
    "    \n",
    "    '''for j,k,l in zip(epsilon_means_non,lambdar_means_non,color_means_non):\n",
    "        ax1.annotate('%s' %l, xy=(j,k), xytext=(j,k))'''\n",
    "        \n",
    "        \n",
    "    \n",
    "    ax1.set_xlim([0,0.9])#[max(m20_means),min(m20_means)])\n",
    "    ax1.set_ylim([0,0.8])#ax1.set_ylim([0.3,0.8])\n",
    "    ax1.set_xlabel(r'$\\epsilon$', size=20)\n",
    "    ax1.set_ylabel('$\\lambda_{\\mathrm{R_e}}$', size=20)\n",
    "    #ax1.set_aspect(abs(max(m20_means)-min(m20_means))/abs(min(gini_means)-max(gini_means)))\n",
    "\n",
    "\n",
    "\n",
    "    #ax1.set_title('Mergers', loc='right')\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "    plt.savefig('Figures/time_evo_epsilon_lambdar_'+str(run)+'.pdf', bbox_inches=\"tight\")\n",
    "    \n",
    "    plt.clf()\n",
    "    fig=plt.figure()\n",
    "    ax1=fig.add_subplot(111)\n",
    "    #ax1.plot(dashed_line_x, dashed_line_y, ls='--', color='black')\n",
    "    #ax1.plot(dashed_line_x_2, dashed_line_y_2, ls='--', color='black')\n",
    "\n",
    "\n",
    "\n",
    "        #ax1.scatter(m20[myr[k]],gini[myr[k]],color=tableau20[k], label=myr[k])\n",
    "    #print(np.shape(m20),np.shape(myr))\n",
    "    #ax1.scatter(m20,gini,c=myr)\n",
    "    #plt.colorbar(im1)\n",
    "    \n",
    "    \n",
    "    im1=ax1.scatter(dPA_means,A2_means,  s=35,c=color_means, cmap='Reds', edgecolors= \"red\")\n",
    "    cb=plt.colorbar(im1,orientation='vertical')\n",
    "    ax1.annotate('Time [Gyr]', xy=(1.08,1.05), xycoords='axes fraction', size=20)\n",
    "    cb.ax.tick_params(labelsize=20)\n",
    "    \n",
    "    \n",
    "    '''for j,k,l in zip(epsilon_means,lambdar_means,color_means):\n",
    "        ax1.annotate('%s' %l, xy=(j,k), xytext=(j,k))'''\n",
    "        \n",
    "    \n",
    "    im2=ax1.scatter(dPA_means_non,A2_means_non,  s=35,c=color_means_non, cmap='Blues', edgecolors= \"blue\")\n",
    "    cb2=plt.colorbar(im2,orientation='vertical')\n",
    "    cb2.ax.tick_params(labelsize=20)\n",
    "    \n",
    "    \n",
    "    '''for j,k,l in zip(epsilon_means_non,lambdar_means_non,color_means_non):\n",
    "        ax1.annotate('%s' %l, xy=(j,k), xytext=(j,k))'''\n",
    "        \n",
    "        \n",
    "    \n",
    "    #ax1.set_xlim([0,0.9])#[max(m20_means),min(m20_means)])\n",
    "    #ax1.set_ylim([0,0.8])#ax1.set_ylim([0.3,0.8])\n",
    "    ax1.set_xlabel(r'$\\Delta$PA', size=20)\n",
    "    \n",
    "    ax1.set_ylabel('$A_2$', size=20)\n",
    "    #ax1.set_aspect(abs(max(m20_means)-min(m20_means))/abs(min(gini_means)-max(gini_means)))\n",
    "\n",
    "\n",
    "\n",
    "    #ax1.set_title('Mergers', loc='right')\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "    plt.savefig('Figures/time_evo_Delta_PA_A2_'+str(run)+'.pdf', bbox_inches=\"tight\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''End of diagnostic plotting'''\n",
    "    \n",
    "    \n",
    "    ct_1=['Delta PA','Delta PA','Delta PA','Delta PA','Delta PA','Delta PA','Delta PA','Delta PA','Delta PA',\n",
    "          'v_asym','v_asym','v_asym','v_asym','v_asym','v_asym','v_asym','v_asym',\n",
    "      's_asym','s_asym','s_asym','s_asym','s_asym','s_asym','s_asym',\n",
    "      'resids','resids','resids','resids','resids','resids',\n",
    "      'lambda_r','lambda_r','lambda_r','lambda_r','lambda_r',\n",
    "         'epsilon','epsilon','epsilon','epsilon',\n",
    "         'A_2','A_2','A_2',\n",
    "         'deltapos','deltapos',\n",
    "         'deltapos2']\n",
    "    ct_2=['v_asym','s_asym','resids','lambda_r','epsilon','A_2','deltapos','deltapos2',\n",
    "      's_asym','resids','lambda_r','epsilon','A_2','deltapos','deltapos2',\n",
    "      'resids','lambda_r','epsilon','A_2','deltapos','deltapos2',\n",
    "      'lambda_r','epsilon','A_2','deltapos','deltapos2',\n",
    "        'epsilon', 'A_2','deltapos','deltapos2',\n",
    "         'A_2','deltapos','deltapos2',\n",
    "         'deltapos','deltapos2',\n",
    "         'deltapos2']\n",
    "    term=['dPA*va','dPA*sa','dPA*resids','dPA*lambdar','dPA*epsilon','dPA*A2','dPA*deltapos','dPA*deltapos2',\n",
    "            'va*resids','va*resids','va*lambdar','va*epsilon','va*A2','va*deltapos','va*deltapos2',\n",
    "          'sa*resids','sa*lambdar','sa*epsilon','sa*A2','sa*deltapos','sa*deltapos2',\n",
    "           'resids*lambdar','resids*epsilon','resids*A2','resids*deltapos','resids*deltapos2',\n",
    "           'lambdar*epsilon','lambdar*A2','lambdar*deltapos','lambdar*deltapos2',\n",
    "           'epsilon*A2','epsilon*deltapos','epsilon*deltapos2',\n",
    "         'A2*deltapos','A2*deltapos2',\n",
    "         'deltapos*deltapos2']\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    inputs=['Delta PA','v_asym','s_asym','resids','lambda_r','epsilon','A_2','deltapos','deltapos2',\n",
    "            'dPA*va','dPA*sa','dPA*resids','dPA*lambdar','dPA*epsilon','dPA*A2','dPA*deltapos','dPA*deltapos2',\n",
    "            'va*sa','va*resids','va*lambdar','va*epsilon','va*A2','va*deltapos','va*deltapos2',\n",
    "            'sa*resids','sa*lambdar','sa*epsilon','sa*A2','sa*deltapos','sa*deltapos2',\n",
    "           'resids*lambdar','resids*epsilon','resids*A2','resids*deltapos','resids*deltapos2',\n",
    "           'lambdar*epsilon','lambdar*A2','lambdar*deltapos','lambdar*deltapos2',\n",
    "           'epsilon*A2','epsilon*deltapos','epsilon*deltapos2',\n",
    "           'A2*deltapos','A2*deltapos2',\n",
    "           'deltapos*deltapos2']\n",
    "            \n",
    "        \n",
    "    \n",
    "    OG_length=len(inputs)\n",
    "    \n",
    "    prev_input=[]\n",
    "    prev_input_here=[]\n",
    "    missclass=[]\n",
    "    missclass_e=[]\n",
    "    num_comps=[]\n",
    "    list_coef=[]\n",
    "    list_coef_std=[]\n",
    "    list_inter=[]\n",
    "    list_inter_std=[]\n",
    "    list_master=[]\n",
    "    list_master_confusion=[]\n",
    "    list_classes=[]\n",
    "    kf_choose=[]\n",
    "    \n",
    "    \n",
    "    kf = StratifiedKFold(n_splits=10, random_state=True, shuffle=True)\n",
    "    \n",
    "    for o in range(len(inputs)):#len(inputs)-20):\n",
    "    \n",
    "        coef_mean=[]\n",
    "        coef_std=[]\n",
    "        inter_mean=[]\n",
    "        inter_std=[]\n",
    "        coef_mean_std=[]\n",
    "        accuracy=[]\n",
    "        accuracy_e=[]\n",
    "        inputs_this_step=[]\n",
    "        confusion_master_this_step=[]\n",
    "        master_this_step=[]\n",
    "        classes_this_step=[]\n",
    "        score_ANN=[]\n",
    "        kf_list=[]\n",
    "        \n",
    "\n",
    "\n",
    "        #Now inputs is changing and you need to go through and choose a variable\n",
    "        for k in range(len(inputs)):#Search through every one\n",
    "            \n",
    "            prev_input.append(inputs[k])\n",
    "            inputs_here=[]\n",
    "            inputs_here.append(inputs[k])\n",
    "            \n",
    "            #print('starting input', prev_input,inputs[k])\n",
    "            #if inputs[k] is a cross term and the prev_input doesn't contain it, add it:\n",
    "            for m in range(len(term)):\n",
    "                if inputs[k]==term[m]:\n",
    "                    #then you are going to search every term of prev_input and see if it is there\n",
    "                    #for n in range(len(prev_input)):\n",
    "                    if ct_1[m] not in prev_input:\n",
    "                        prev_input.append(ct_1[m])\n",
    "                        \n",
    "                        inputs_here.append(ct_1[m])\n",
    "                    if ct_2[m] not in prev_input:\n",
    "                        prev_input.append(ct_2[m])\n",
    "                        inputs_here.append(ct_2[m])\n",
    "                        \n",
    "                    \n",
    "            #print('inputs heading into LDA',prev_input)\n",
    "            \n",
    "            X = df[prev_input].values\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            y = df['class label'].values\n",
    "\n",
    "            from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "            std_scale = preprocessing.StandardScaler().fit(X)\n",
    "            X = std_scale.transform(X)\n",
    "\n",
    "\n",
    "            enc = LabelEncoder()\n",
    "            label_encoder = enc.fit(y)\n",
    "            y = label_encoder.transform(y) + 1\n",
    "\n",
    "\n",
    "            label_dict = {1: 'NonMerger', 2: 'Merger'}\n",
    "            from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "            # LDA\n",
    "            sklearn_lda = LDA(priors=priors_list[i], store_covariance=True)#store_covariance=False\n",
    "\n",
    "\n",
    "            X_lda_sklearn = sklearn_lda.fit_transform(X, y)\n",
    "\n",
    "            dec = sklearn_lda.score(X,y)\n",
    "            prob = sklearn_lda.predict_proba(X)\n",
    "\n",
    "            coef = sklearn_lda.coef_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "            kf.get_n_splits(X, y)\n",
    "            kf_list.append(kf.split(X,y))\n",
    "\n",
    "\n",
    "\n",
    "            coef_list=[]\n",
    "            inter_list=[]\n",
    "            classes_list=[]\n",
    "            confusion_master=[]\n",
    "            y_test_master=[]\n",
    "            pred_master=[]\n",
    "            single_prediction=[]\n",
    "            score=[]\n",
    "            count=0\n",
    "            for train_index, test_index in kf.split(X, y):\n",
    "\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "                sklearn_lda = LDA( priors=priors_list[i],store_covariance=True)#store_covariance=False\n",
    "            \n",
    "\n",
    "\n",
    "                X_lda_sklearn = sklearn_lda.fit_transform(X_train, y_train)\n",
    "                coef = sklearn_lda.coef_\n",
    "                inter = sklearn_lda.intercept_\n",
    "\n",
    "                inter_list.append(inter)\n",
    "                coef_list.append(coef)\n",
    "                inter_list.append(inter)\n",
    "                pred =sklearn_lda.predict(X_test)\n",
    "                classes_list.append(sklearn_lda.classes_)\n",
    "                confusion_master.append(confusion_matrix(pred,y_test))\n",
    "\n",
    "                single_prediction.append(confusion_matrix(pred,y_test)[1][0]+confusion_matrix(pred,y_test)[0][1])\n",
    "\n",
    "\n",
    "\n",
    "            score_ANN.append(np.mean(score))\n",
    "            accuracy.append(np.mean(single_prediction))#/(master[0][0]+master[1][0]+master[0][1]+master[1][1]))\n",
    "            \n",
    "            accuracy_e.append(np.std(single_prediction))\n",
    "            inputs_this_step.append(np.array(prev_input))\n",
    "            \n",
    "            confusion_master_this_step.append(np.array((np.mean(confusion_master,axis=0)/np.sum(np.mean(confusion_master,axis=0))).transpose()))\n",
    "            master_this_step.append(np.array(np.mean(confusion_master, axis=0).transpose()))\n",
    "            #print('appending with this', np.array(prev_input))\n",
    "            \n",
    "            classes_this_step.append(np.array(classes_list))\n",
    "    \n",
    "            \n",
    "            coef_mean.append(np.mean(coef_list, axis=0))\n",
    "            coef_std.append(np.std(coef_list, axis=0))\n",
    "            \n",
    "            inter_mean.append(np.mean(inter_list, axis=0))\n",
    "            inter_std.append(np.std(inter_list, axis=0))\n",
    "            \n",
    "            #prev_input.remove(new_stuff)\n",
    "            for m in range(len(inputs_here)):\n",
    "                try:\n",
    "                    prev_input.remove(inputs_here[m])\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        \n",
    "        try:\n",
    "            if accuracy_e[accuracy.index(min(accuracy))]<0.00001:\n",
    "                break\n",
    "        except ValueError:\n",
    "            continue\n",
    "        #print('all of inputs', inputs_this_step)\n",
    "        #print('selecting the best model for this step', (inputs_this_step[accuracy.index(min(accuracy))]))\n",
    "        \n",
    "        thing=(inputs_this_step[accuracy.index(min(accuracy))])\n",
    "        first_A=min(accuracy)\n",
    "        \n",
    "        prev_input_here.append(thing)\n",
    "        \n",
    "        for m in range(len(thing)):\n",
    "            \n",
    "            prev_input.append(thing[m])\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                inputs.remove(thing[m])\n",
    "            except ValueError:\n",
    "                #print('~~~RUning into troubles')\n",
    "                #print('inputs', inputs)\n",
    "                #print('the thing to remove', thing[m])\n",
    "                continue\n",
    "        #print('the input now', inputs)\n",
    "        #STOP\n",
    "        prev_input=list(set(prev_input))\n",
    "        missclass.append(min(accuracy))\n",
    "        #print('coef previous to selecting min', coef_mean)\n",
    "        missclass_e.append(accuracy_e[accuracy.index(min(accuracy))])\n",
    "        \n",
    "        kf_choose.append(kf_list[accuracy.index(min(accuracy))])\n",
    "        \n",
    "        list_coef.append(coef_mean[accuracy.index(min(accuracy))])\n",
    "        #print('coef list', coef_mean[accuracy.index(min(accuracy))])\n",
    "        list_coef_std.append(coef_std[accuracy.index(min(accuracy))])\n",
    "        \n",
    "        list_inter.append(inter_mean[accuracy.index(min(accuracy))])\n",
    "        list_inter_std.append(inter_std[accuracy.index(min(accuracy))])\n",
    "        \n",
    "        list_master.append(master_this_step[accuracy.index(min(accuracy))])\n",
    "        list_master_confusion.append(confusion_master_this_step[accuracy.index(min(accuracy))])\n",
    "        \n",
    "        list_classes.append(classes_this_step[accuracy.index(min(accuracy))])\n",
    "        \n",
    "        #list_score.append(score_ANN[accuracy.index(min(accuracy))])\n",
    "        \n",
    "        num_comps.append(len(prev_input))#\n",
    "        #print('min index', accuracy.index(min(accuracy)))\n",
    "        #if OG_length < len(:\n",
    "        #    break\n",
    "        \n",
    "        if len(thing)==28:\n",
    "            break\n",
    "        \n",
    "    #print('these are your inputs',prev_input, prev_input_here)\n",
    "    \n",
    "    \n",
    "    #list_coef=[]\n",
    "    #list_coef_std=[]\n",
    "    \n",
    "    min_A=min(missclass)\n",
    "    min_comps=num_comps[missclass.index(min(missclass))]\n",
    "    for p in range(len(missclass)):\n",
    "        missclass_list.append(missclass[p])\n",
    "    \n",
    "    min_index=locate_min(missclass)[1][0]\n",
    "    \n",
    "    \n",
    "    min_A=missclass[locate_min(missclass)[1][0]]\n",
    "    min_A_e=missclass_e[locate_min(missclass)[1][0]]\n",
    "    '''Now you need to use one standard error '''\n",
    "    \n",
    "    \n",
    "    for m in range(len(missclass)):\n",
    "        if (missclass[m]) < (min_A+min_A_e):\n",
    "            print('m',missclass[m],min_A+min_A_e)\n",
    "            new_min_index=m\n",
    "            break\n",
    "        else:\n",
    "            new_min_index=min_index\n",
    "            \n",
    "    min_A=missclass[new_min_index]\n",
    "    min_A_e=missclass_e[new_min_index]\n",
    "    min_comps=num_comps[new_min_index]\n",
    "    inputs_all=prev_input_here[new_min_index]#:new_min_index+1]\n",
    "    \n",
    "    splits=kf_choose[new_min_index]\n",
    "    \n",
    "    X = df[inputs_all].values\n",
    "    y = df['class label'].values\n",
    "\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "    std_scale = preprocessing.StandardScaler().fit(X)\n",
    "    X = std_scale.transform(X)\n",
    "    \n",
    "    '''This is the part where we test logistic regression'''\n",
    "    '''But how to compare the number of misclassifications to that of LDA?'''\n",
    "    enc = LabelEncoder()\n",
    "    label_encoder = enc.fit(y)\n",
    "    y = label_encoder.transform(y)\n",
    "    logistic = sm.Logit(y, X).fit()\n",
    "    print(logistic.summary())\n",
    "    print('this is the result of logistic')\n",
    "    \n",
    "    '''Gotta split up again into k splits'''\n",
    "    kf.get_n_splits(X, y)\n",
    "    confusion_master =[]\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "\n",
    "        pred =logistic.predict(X_test)\n",
    "        pred_binary = []\n",
    "        for j in range(len(pred)):\n",
    "            if pred[j] > 0.5:\n",
    "                pred_binary.append(1)\n",
    "            else:\n",
    "                pred_binary.append(0)\n",
    "        confusion_master.append(confusion_matrix(pred_binary,y_test))\n",
    "    confusion_master_this_step = np.array((np.mean(confusion_master,axis=0)/np.sum(np.mean(confusion_master,axis=0))).transpose())\n",
    "    #what is going on?\n",
    "    plt.clf()\n",
    "    fig=plt.figure()#figsize=(6,6)\n",
    "    plot_confusion_matrix(confusion_master_this_step, sklearn_lda.classes_, title='Normalized Confusion Matrix')\n",
    "    plt.savefig('Figures/Confusion_matrix_logistic_'+str(run)+'.pdf')\n",
    "    #This is from this website: http://www.science.smith.edu/~jcrouser/SDS293/labs/lab5-py.html\n",
    "    plt.clf()\n",
    "    \n",
    "    \n",
    "    \n",
    "    y = df['class label'].values\n",
    "    enc = LabelEncoder()\n",
    "    label_encoder = enc.fit(y)\n",
    "    y = label_encoder.transform(y) + 1\n",
    "\n",
    "\n",
    "    label_dict = {1: 'NonMerger', 2: 'Merger'}\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "    score=[]\n",
    "    score_QDA=[]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(prev_input)\n",
    "    print('terms before adding in necessary', inputs_all)\n",
    "    print(run,'&')\n",
    "    print('coefficients', list_coef[new_min_index])\n",
    "    print('coefficient std', list_coef_std[new_min_index])\n",
    "    \n",
    "    \n",
    "    '''write a nice thing to output in the proper format'''\n",
    "    dPA_c='--'\n",
    "    v_asym_c='--'\n",
    "    s_asym_c='--'\n",
    "    resids_c='--'\n",
    "    lambdar_c='--'\n",
    "    epsilon_c='--'\n",
    "    A2_c='--'\n",
    "    deltapos_c='--'\n",
    "    deltapos2_c='--'\n",
    "    \n",
    "    dPA_v_asym_c='--'\n",
    "    dPA_s_asym_c='--'\n",
    "    dPA_resids_c='--'\n",
    "    dPA_lambdar_c='--'\n",
    "    dPA_epsilon_c='--'\n",
    "    dPA_A2_c='--'\n",
    "    dPA_deltapos_c='--'\n",
    "    dPA_deltapos2_c='--'\n",
    "    \n",
    "    v_asym_s_asym_c='--'\n",
    "    v_asym_resids_c='--'\n",
    "    v_asym_lambdar_c='--'\n",
    "    v_asym_epsilon_c='--'\n",
    "    v_asym_A2_c='--'\n",
    "    v_asym_deltapos_c='--'\n",
    "    v_asym_deltapos2_c='--'\n",
    "    \n",
    "    s_asym_resids_c='--'\n",
    "    s_asym_lambdar_c='--'\n",
    "    s_asym_epsilon_c='--'\n",
    "    s_asym_A2_c='--'\n",
    "    s_asym_deltapos_c='--'\n",
    "    s_asym_deltapos2_c='--'\n",
    "    \n",
    "    resids_lambdar_c='--'\n",
    "    resids_epsilon_c='--'\n",
    "    resids_A2_c='--'\n",
    "    resids_deltapos_c='--'\n",
    "    resids_deltapos2_c='--'\n",
    "    \n",
    "    lambdar_epsilon_c='--'\n",
    "    lambdar_A2_c='--'\n",
    "    lambdar_deltapos_c='--'\n",
    "    lambdar_deltapos2_c='--'\n",
    "    \n",
    "    epsilon_A2_c='--'\n",
    "    epsilon_deltapos_c='--'\n",
    "    epislon_deltapos2_c='--'\n",
    "    \n",
    "    A2_deltapos_c='--'\n",
    "    A2_deltapos2_c='--'\n",
    "    \n",
    "    deltapos_deltapos2_c='--'\n",
    "    \n",
    "    \n",
    "    for p in range(len(prev_input_here[new_min_index])):\n",
    "        \n",
    "        if str(prev_input_here[new_min_index][p])=='Delta PA':\n",
    "            dPA_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        \n",
    "        if str(prev_input_here[new_min_index][p])=='s_asym':\n",
    "            s_asym_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        \n",
    "        if str(prev_input_here[new_min_index][p])=='resids':\n",
    "            resids_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        \n",
    "        if str(prev_input_here[new_min_index][p])=='lambda_r':\n",
    "            lambdar_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        \n",
    "        if str(prev_input_here[new_min_index][p])=='epsilon':\n",
    "            epsilon_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        \n",
    "        if str(prev_input_here[new_min_index][p])=='A_2':\n",
    "            A2_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        \n",
    "        if str(prev_input_here[new_min_index][p])=='deltapos':\n",
    "            deltapos_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        \n",
    "        \n",
    "        \n",
    "        if str(prev_input_here[new_min_index][p])=='dPA*sa':\n",
    "            dPA_s_asym_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        if str(prev_input_here[new_min_index][p])=='dPA*resids':\n",
    "            dPA_resids_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        if str(prev_input_here[new_min_index][p])=='dPA*lambdar':\n",
    "            dPA_lambdar_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        if str(prev_input_here[new_min_index][p])=='dPA*epsilon':\n",
    "            dPA_epsilon_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        if str(prev_input_here[new_min_index][p])=='dPA*A2':\n",
    "            dPA_A2_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        if str(prev_input_here[new_min_index][p])=='dPA*deltapos':\n",
    "            dPA_deltapos_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        \n",
    "        if str(prev_input_here[new_min_index][p])=='sa*resids':\n",
    "            s_asym_resids_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        if str(prev_input_here[new_min_index][p])=='sa*lambdar':\n",
    "            s_asym_lambdar_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        if str(prev_input_here[new_min_index][p])=='sa*epsilon':\n",
    "            s_asym_epsilon_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        if str(prev_input_here[new_min_index][p])=='sa*A2':\n",
    "            s_asym_A2_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        if str(prev_input_here[new_min_index][p])=='sa*deltapos':\n",
    "            s_asym_deltapos_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        \n",
    "        if str(prev_input_here[new_min_index][p])=='resids*lambdar':\n",
    "            resids_lambdar_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        if str(prev_input_here[new_min_index][p])=='resids*epsilon':\n",
    "            resids_epsilon_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        if str(prev_input_here[new_min_index][p])=='resids*A2':\n",
    "            resids_A2_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        if str(prev_input_here[new_min_index][p])=='resids*deltapos':\n",
    "            resids_deltapos_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        \n",
    "        if str(prev_input_here[new_min_index][p])=='lambdar*epsilon':\n",
    "            lambdar_epsilon_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        if str(prev_input_here[new_min_index][p])=='lambdar*A2':\n",
    "            lambdar_A2_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        if str(prev_input_here[new_min_index][p])=='lambdar*deltapos':\n",
    "            lambdar_deltapos_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        \n",
    "        \n",
    "        if str(prev_input_here[new_min_index][p])=='epsilon*A2':\n",
    "            epsilon_A2_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        if str(prev_input_here[new_min_index][p])=='epsilon*deltapos':\n",
    "            epsilon_deltapos_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        \n",
    "        if str(prev_input_here[new_min_index][p])=='A2*deltapos':\n",
    "            A2_deltapos_c='\\\\'+'textbf{'+str(round(list_coef[new_min_index][0][p],2))+' $\\pm$ '+str(round(list_coef_std[new_min_index][0][p],2))+'}'\n",
    "        \n",
    "        \n",
    "        \n",
    "    print('~~~~~')\n",
    "    print(str(run)+' & '+dPA_c+' & '+s_asym_c+' & '+resids_c+' & '+lambdar_c+' & '+epsilon_c+' & '+A2_c+' & '+deltapos_c+' & '+'\\\\'+'textbf{'+str(round(list_inter[new_min_index][0],2))+' $\\pm$ '+str(round(list_inter_std[new_min_index][0],2))+'}'+'\\\\'+'\\\\')\n",
    "        \n",
    "    print(str(run)+' & '+dPA_s_asym_c+' & '+dPA_resids_c+' & '+dPA_lambdar_c+' & '+dPA_epsilon_c+' & '+dPA_A2_c+' & '+dPA_deltapos_c+'\\\\'+'\\\\')\n",
    "    print(str(run)+' & '+s_asym_resids_c+' & '+s_asym_lambdar_c+' & '+s_asym_epsilon_c+' & '+s_asym_A2_c+' & '+s_asym_deltapos_c+'\\\\'+'\\\\')\n",
    "    print(str(run)+' & '+resids_lambdar_c+' & '+resids_epsilon_c+' & '+resids_A2_c+' & '+resids_deltapos_c+' & '+lambdar_epsilon_c+' & '+lambdar_A2_c+' & '+lambdar_deltapos_c+' & '+epsilon_A2_c+' & '+epsilon_deltapos_c+' & '+A2_deltapos_c+'\\\\'+'\\\\')\n",
    "    \n",
    "    \n",
    "    print('~~~~~')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print('intercept', list_inter[new_min_index])\n",
    "    print('intercept std', list_inter_std[new_min_index])\n",
    "    \n",
    "    \n",
    "    plt.clf()\n",
    "    fig=plt.figure()#figsize=(6,6)\n",
    "    plot_confusion_matrix(list_master_confusion[new_min_index], sklearn_lda.classes_, title='Normalized Confusion Matrix')\n",
    "    plt.savefig('Figures/Confusion_matrix_kin_'+str(run)+'.pdf')\n",
    "    #This is from this website: http://www.science.smith.edu/~jcrouser/SDS293/labs/lab5-py.html\n",
    "    plt.clf()\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    \n",
    "    master=list_master[new_min_index]\n",
    "    \n",
    "    print('~~~Accuracy~~~')\n",
    "    print((master[1][1]+master[0][0])/(master[0][0]+master[1][0]+master[0][1]+master[1][1]))\n",
    "    print('~~~Precision~~~')\n",
    "    print(master[1][1]/(master[0][1]+master[1][1]))#TP/(TP+FP)\n",
    "    print('~~~Recall~~~')\n",
    "    print(master[1][1]/(master[1][0]+master[1][1]))#TP/(TP+FN)\n",
    "    print('~~~F1~~~')\n",
    "    print((2*master[1][1])/(master[0][1]+master[1][0]+2*master[1][1]))#2TP/(2TP+FP+FN)\n",
    "    \n",
    "    X = df[inputs_all].values\n",
    "    y = df['class label'].values\n",
    "    std_scale = preprocessing.StandardScaler().fit(X)\n",
    "    X = std_scale.transform(X)\n",
    "\n",
    "\n",
    "    enc = LabelEncoder()\n",
    "    label_encoder = enc.fit(y)\n",
    "    y = label_encoder.transform(y) + 1\n",
    "\n",
    "\n",
    "    label_dict = {1: 'NonMerger', 2: 'Merger'}\n",
    "    sklearn_lda = LDA(priors=priors_list[i], store_covariance=True)#store_covariance=False\n",
    "\n",
    "\n",
    "    \n",
    "    X_lda_sklearn = sklearn_lda.fit_transform(X, y)\n",
    "    \n",
    "    X_lda_1=[]\n",
    "    X_lda_2=[]\n",
    "    for j in range(len(X_lda_sklearn)):\n",
    "        if y[j] ==1:\n",
    "            X_lda_1.append(X_lda_sklearn[j][0])\n",
    "        else:\n",
    "            X_lda_2.append(X_lda_sklearn[j][0])\n",
    "    input_hist=X_lda_sklearn\n",
    "    \n",
    "    \n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(20,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.hist(X_lda_1, label='Nonmerger',  color='blue', alpha=0.5,normed=1)\n",
    "    ax.hist(X_lda_2, label='Merger',  color=colors[i],alpha = 0.5,normed=1)#sns.xkcd_rgb[\"salmon\"]\n",
    "\n",
    "    \n",
    "\n",
    "    ax.set_xlabel('LD1', size=30)\n",
    "    #ax.set_title('Histogram #%s' %str(cnt+1), size=20)\n",
    "\n",
    "    # hide axis ticks\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n",
    "            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\", labelsize=30)\n",
    "\n",
    "    # remove axis spines\n",
    "    ax.spines[\"top\"].set_visible(False)  \n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"bottom\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)    \n",
    "\n",
    "    ax.set_ylabel('Relative Count', size=30)\n",
    "    \n",
    "    \n",
    "    plt.legend(loc=\"upper right\", fontsize=30)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    xs=[]\n",
    "    LDA1=[]\n",
    "    \n",
    "    my_lists = {key:[] for key in myr}\n",
    "    my_lists_non = {key:[] for key in myr_non}\n",
    "    my_lists_none = []\n",
    "    my_lists_merg = []\n",
    "    separations = {key:[] for key in myr}\n",
    "\n",
    "    \n",
    "    \n",
    "    for j in range(len(df)):\n",
    "        if df[['class label']].values[j]==0:\n",
    "            my_lists_none.append(X_lda_sklearn[j][0])\n",
    "            my_lists_non[df[['Myr']].values[j][0]].append(X_lda_sklearn[j][0])\n",
    "            continue\n",
    "        else:\n",
    "            my_lists_merg.append(X_lda_sklearn[j][0])\n",
    "            \n",
    "            my_lists[df[['Myr']].values[j][0]].append(X_lda_sklearn[j][0])\n",
    "        \n",
    "            L=X_lda_sklearn[j][0]\n",
    "            #df[['Gini']].values[i][0]*coef[0][0]+df[['M20']].values[i][0]*coef[0][1]+df[['Concentration (C)']].values[i][0]*coef[0][2]+df[['Asymmetry (A)']].values[i][0]*coef[0][3]+df[['Clumpiness (S)']].values[i][0]*coef[0][4]+df[['Sersic N']].values[i][0]*coef[0][5]+df[['Shape Asymmetry']].values[i][0]*coef[0][6]\n",
    "            LDA1.append(L)\n",
    "            xs.append(df[['Myr']].values[j][0])\n",
    "\n",
    "    #print(mean(my_lists[180]))\n",
    "    mean_non=(np.mean(my_lists_merg)+np.mean(my_lists_none))/2\n",
    "    \n",
    "    \n",
    "    print('DECISION BOUNDARY', mean_non, run)\n",
    "    if run[:7]=='fg3_m13':\n",
    "        plt.annotate('q0.333_fg0.3', xy=(0.01,0.85), xycoords='axes fraction', size=30)\n",
    "    \n",
    "    if run[:7]=='fg1_m13':\n",
    "        plt.annotate('q0.333_fg0.1', xy=(0.01,0.85), xycoords='axes fraction', size=30)\n",
    "    if run[:7]=='fg3_m12':\n",
    "        plt.annotate('q0.5_fg0.3', xy=(0.01,0.85), xycoords='axes fraction', size=30)\n",
    "    if run[:7]=='fg3_m15':\n",
    "        plt.annotate('q0.2_fg0.3_BT0.2', xy=(0.01,0.85), xycoords='axes fraction', size=30)\n",
    "    if run[:9]=='major_all':\n",
    "        plt.annotate('Major Mergers', xy=(0.01,0.85), xycoords='axes fraction', size=30)\n",
    "    if run[:9]=='minor_all':\n",
    "        plt.annotate('Minor Mergers', xy=(0.01,0.85), xycoords='axes fraction', size=30)\n",
    "    \n",
    "    if run[:9]=='fg3_m1_10':\n",
    "        plt.annotate('q0.1_fg0.3_BT0.2', xy=(0.01,0.85), xycoords='axes fraction', size=30)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    means=[]\n",
    "    std=[]\n",
    "    myr_here=[]\n",
    "    #print('separations before averaged', separations)\n",
    "    for j in range(len(myr)):\n",
    "        \n",
    "        if np.std(my_lists[myr[j]])==0:# or np.std(my_lists[myr[j]])< 0.01*np.mean(my_lists[myr[j]]):\n",
    "            continue\n",
    "        means.append(np.mean(my_lists[myr[j]]))\n",
    "        std.append(np.std(my_lists[myr[j]]))\n",
    "        \n",
    "        myr_here.append(myr[j])\n",
    "    print('myr', myr_here)\n",
    "    \n",
    "    myr_total.append(myr_here[-1]-myr_here[0])\n",
    "    \n",
    "    myr_detect_LDA_val=[]\n",
    "    myr_detect_LDA=[]\n",
    "    for o in range(len(means)):\n",
    "        if means[o] > mean_non:#this means above the decision boundary\n",
    "            myr_detect_LDA.append(o)\n",
    "            myr_detect_LDA_val.append(myr[o])\n",
    "    grouped=group_consecutives(myr_detect_LDA)\n",
    "    \n",
    "    interval=[]\n",
    "    for o in range(len(grouped)):\n",
    "        interval.append(myr_detect_LDA_val[myr_detect_LDA.index(grouped[o][-1])]-myr_detect_LDA_val[myr_detect_LDA.index(grouped[o][0])])\n",
    "    LDA_time.append(np.sum(interval))\n",
    "    \n",
    "    \n",
    "    means=np.array(means)\n",
    "    std=np.array(std)\n",
    "    myr_here=np.array(myr_here)\n",
    "    \n",
    "    means_non=[]\n",
    "    std_non=[]\n",
    "    myr_here_non=[]\n",
    "    for j in range(len(myr_non)):\n",
    "        \n",
    "        if np.std(my_lists_non[myr_non[j]])==0 or np.std(my_lists_non[myr_non[j]])< 0.01*np.mean(my_lists_non[myr_non[j]]):\n",
    "            continue\n",
    "        means_non.append(np.mean(my_lists_non[myr_non[j]]))\n",
    "        std_non.append(np.std(my_lists_non[myr_non[j]]))\n",
    "        myr_here_non.append(myr_non[j])\n",
    "        \n",
    "    \n",
    "    means_non=np.array(means_non)\n",
    "    std_non=np.array(std_non)\n",
    "    myr_here_non=np.array(myr_here_non)\n",
    "    \n",
    "    \n",
    "    plt.axvline(x=mean_non, color='black')\n",
    "    plt.tight_layout()\n",
    "    plt.ylim([0,1])\n",
    "    plt.xlim([-3,9])\n",
    "    plt.savefig('Hist_kin_'+str(run)+'.pdf')\n",
    "    plt.clf()\n",
    "    \n",
    "    xs=[]\n",
    "    LDA1=[]\n",
    "    \n",
    "    my_lists = {key:[] for key in myr}\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    my_lists_non = {key:[] for key in myr_non}\n",
    "    my_lists_none = []\n",
    "    my_lists_merg = []\n",
    "    separations = {key:[] for key in myr}\n",
    "\n",
    "    \n",
    "    \n",
    "    for j in range(len(df)):\n",
    "        if df[['class label']].values[j]==0:\n",
    "            my_lists_none.append(X_lda_sklearn[j][0])\n",
    "            my_lists_non[df[['Myr']].values[j][0]].append(X_lda_sklearn[j][0])\n",
    "            \n",
    "            \n",
    "            continue\n",
    "        else:\n",
    "            my_lists_merg.append(X_lda_sklearn[j][0])\n",
    "        \n",
    "            my_lists[df[['Myr']].values[j][0]].append(X_lda_sklearn[j][0])\n",
    "            L=X_lda_sklearn[j][0]\n",
    "            #df[['Gini']].values[i][0]*coef[0][0]+df[['M20']].values[i][0]*coef[0][1]+df[['Concentration (C)']].values[i][0]*coef[0][2]+df[['Asymmetry (A)']].values[i][0]*coef[0][3]+df[['Clumpiness (S)']].values[i][0]*coef[0][4]+df[['Sersic N']].values[i][0]*coef[0][5]+df[['Shape Asymmetry']].values[i][0]*coef[0][6]\n",
    "            LDA1.append(L)\n",
    "            xs.append(df[['Myr']].values[j][0])\n",
    "\n",
    "    #print(mean(my_lists[180]))\n",
    "    mean_non=(np.mean(my_lists_merg)+np.mean(my_lists_none))/2\n",
    "    \n",
    "    \n",
    "    print('DECISION BOUNDARY', mean_non, run)\n",
    "    \n",
    "    \n",
    "    \n",
    "    means=[]\n",
    "    std=[]\n",
    "    myr_here=[]\n",
    "    \n",
    "    for j in range(len(myr)):\n",
    "        \n",
    "        if np.std(my_lists[myr[j]])==0:# or np.std(my_lists[myr[j]])< 0.01*np.mean(my_lists[myr[j]]):\n",
    "            continue\n",
    "        means.append(np.mean(my_lists[myr[j]]))#was just my_lists\n",
    "        std.append(np.std(my_lists[myr[j]]))\n",
    "        \n",
    "   \n",
    "        \n",
    "        myr_here.append(myr[j])\n",
    "    print('myr', myr_here)\n",
    "    \n",
    "    myr_total.append(myr_here[-1]-myr_here[0])\n",
    "    \n",
    "    myr_detect_LDA_val=[]\n",
    "    myr_detect_LDA=[]\n",
    "    for o in range(len(means)):\n",
    "        if means[o] - std[o] > mean_non:#this means above the decision boundary\n",
    "            myr_detect_LDA.append(o)\n",
    "            myr_detect_LDA_val.append(myr[o])\n",
    "    '''grouped=group_consecutives(myr_detect_LDA)\n",
    "    print('grouped', grouped)\n",
    "    interval=[]\n",
    "    for o in range(len(grouped)):\n",
    "        interval.append(myr_detect_LDA_val[myr_detect_LDA.index(grouped[o][-1])]-myr_detect_LDA_val[myr_detect_LDA.index(grouped[o][0])])\n",
    "    #LDA_time.append(np.sum(interval))\n",
    "    '''\n",
    "    \n",
    "    means=np.array(means)\n",
    "    std=np.array(std)\n",
    "    myr_here=np.array(myr_here)\n",
    "    \n",
    "    means_non=[]\n",
    "    std_non=[]\n",
    "    myr_here_non=[]\n",
    "    \n",
    "    \n",
    "    for j in range(len(myr_non)):\n",
    "        \n",
    "        if np.std(my_lists_non[myr_non[j]])==0 or np.std(my_lists_non[myr_non[j]])< 0.01*np.mean(my_lists_non[myr_non[j]]):\n",
    "            continue\n",
    "        means_non.append(np.mean(my_lists_non[myr_non[j]]))\n",
    "        std_non.append(np.std(my_lists_non[myr_non[j]]))\n",
    "        myr_here_non.append(myr_non[j])\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    means_non=np.array(means_non)\n",
    "    std_non=np.array(std_non)\n",
    "    myr_here_non=np.array(myr_here_non)\n",
    "    \n",
    "    \n",
    "    \n",
    "    new_means=np.array([x for x in means])\n",
    "    new_means_non=np.array([x for x in means_non])\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    myr_here_non_left=[]\n",
    "    myr_here_non_right=[]\n",
    "    new_means_non_left=[]\n",
    "    new_means_non_right=[]\n",
    "    std_non_left=[]\n",
    "    std_non_right=[]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    for p in range(len(myr_here_non)):\n",
    "        if myr_here_non[p] > 2:\n",
    "            myr_here_non_right.append(myr_here_non[p])\n",
    "            new_means_non_right.append(new_means_non[p])\n",
    "            std_non_right.append(std_non[p])\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "        else:\n",
    "            myr_here_non_left.append(myr_here_non[p])\n",
    "            new_means_non_left.append(new_means_non[p])\n",
    "            std_non_left.append(std_non[p])\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "    myr_here_non_right=np.array(myr_here_non_right)\n",
    "    myr_here_non_left=np.array(myr_here_non_left)\n",
    "    new_means_non_right=np.array(new_means_non_right)\n",
    "    new_means_non_left=np.array(new_means_non_left)\n",
    "    std_non_right=np.array(std_non_right)\n",
    "    std_non_left=np.array(std_non_left)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    plt.clf()\n",
    "    fig = plt.figure(figsize=(20,5))\n",
    "    \n",
    "    new_means=np.array([x for x in means])\n",
    "    new_means_non=np.array([x for x in means_non])\n",
    "\n",
    "\n",
    "    plt.plot(myr_here, new_means, color=colors[i])\n",
    "    plt.scatter(myr_here, new_means, color=colors[i], s=20)\n",
    "\n",
    "    plt.fill_between(myr_here, (new_means-std), (new_means+std),alpha=.5, color=colors[i])\n",
    "\n",
    "    myr_here_non_left=[]\n",
    "    myr_here_non_right=[]\n",
    "    new_means_non_left=[]\n",
    "    new_means_non_right=[]\n",
    "    std_non_left=[]\n",
    "    std_non_right=[]\n",
    "    for p in range(len(myr_here_non)):\n",
    "        if myr_here_non[p] > 2:\n",
    "            myr_here_non_right.append(myr_here_non[p])\n",
    "            new_means_non_right.append(new_means_non[p])\n",
    "            std_non_right.append(std_non[p])\n",
    "        else:\n",
    "            myr_here_non_left.append(myr_here_non[p])\n",
    "            new_means_non_left.append(new_means_non[p])\n",
    "            std_non_left.append(std_non[p])\n",
    "            \n",
    "    myr_here_non_right=np.array(myr_here_non_right)\n",
    "    myr_here_non_left=np.array(myr_here_non_left)\n",
    "    new_means_non_right=np.array(new_means_non_right)\n",
    "    new_means_non_left=np.array(new_means_non_left)\n",
    "    std_non_right=np.array(std_non_right)\n",
    "    std_non_left=np.array(std_non_left)\n",
    "    plt.plot(myr_here_non_right, new_means_non_right, color='blue')\n",
    "    plt.plot(myr_here_non_left, new_means_non_left, color='blue')\n",
    "    plt.fill_between(myr_here_non_left, (new_means_non_left-std_non_left), (new_means_non_left+std_non_left),alpha=.5, color='blue')\n",
    "    plt.fill_between(myr_here_non_right, (new_means_non_right-std_non_right), (new_means_non_right+std_non_right),alpha=.5, color='blue')\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.scatter(myr_here_non, new_means_non, color='blue',s=20)\n",
    "    \n",
    "    plt.xlim([min([min(myr_non),min(myr)]),max([max(myr_non),max(myr)])]) \n",
    "    \n",
    "    \n",
    "    if run[:7]=='fg1_m13':\n",
    "        \n",
    "        \n",
    "        #plt.axvline(x=220/100, color='black', ls='--')\n",
    "        plt.axvline(x=2.25, color='black', ls='--')\n",
    "        plt.axvline(x=2.74, color='black', ls='--')\n",
    "        plt.axvline(x=2.74+(0.5), color='black', ls='--')\n",
    "\n",
    "        \n",
    "        plt.annotate('q0.333_fg0.1', xy=(0.07,max(new_means+std)-0.35), size=35)\n",
    "        plt.annotate('Early', xy=(1.7,max(new_means+std)-0.35),  size=35)\n",
    "        plt.annotate('Late', xy=(2.3,max(new_means+std)-0.35), size=35)\n",
    "        plt.annotate('Post Coalescence', xy=(2.8,max(new_means+std)-0.35), size=35)\n",
    "        \n",
    "        \n",
    "        \n",
    "    if run[:7]=='fg3_m13':\n",
    "    \n",
    "        \n",
    "        plt.axvline(x=2.4, color='black', ls='--')\n",
    "        plt.axvline(x=2.59, color='black', ls='--')\n",
    "        plt.axvline(x=2.59+0.5, color='black', ls='--')\n",
    "\n",
    "\n",
    "\n",
    "        plt.annotate('q0.333_fg0.3', xy=(0.07,max(new_means+std)-0.35), size=35)\n",
    "        plt.annotate('Early', xy=(1.68,max(new_means+std)-0.35),  size=35)\n",
    "        plt.annotate('Late', xy=(2.45,max(new_means+std)-0.35),size=35)\n",
    "        plt.annotate('Post Coalescence', xy=(2.8,max(new_means+std)-0.35),  size=35)\n",
    "    \n",
    "    if run[:7]=='fg3_m12':\n",
    "        \n",
    "        \n",
    "        plt.axvline(x=2.15, color='black', ls='--')\n",
    "        plt.axvline(x=1.76, color='black', ls='--')\n",
    "        plt.axvline(x=2.15+0.5, color='black', ls='--')\n",
    "\n",
    "\n",
    "\n",
    "        plt.annotate('q0.5_fg0.3', xy=(0.07,max(new_means+std)-0.35), size=35)\n",
    "        plt.annotate('Early', xy=(1.4,max(new_means+std)-0.35),  size=35)\n",
    "        plt.annotate('Late', xy=(1.765,max(new_means+std)-0.35),size=35)\n",
    "        plt.annotate('Post Coalescence', xy=(2.22,max(new_means+std)-0.35),  size=35)\n",
    "    if run[:7]=='fg3_m15':\n",
    "    \n",
    "        plt.axvline(x=3.72, color='black', ls='--')\n",
    "        plt.axvline(x=3.13, color='black', ls='--')\n",
    "        plt.axvline(x=3.72+0.5, color='black', ls='--')\n",
    "\n",
    "        plt.annotate('q0.2_fg0.3_BT0.2', xy=(0.07,max(new_means+std)-0.35), size=35)\n",
    "        plt.annotate('Early', xy=(2,max(new_means+std)-0.35),  size=35)\n",
    "        plt.annotate('Late', xy=(3.3,max(new_means+std)-0.35), size=35)\n",
    "        plt.annotate('Post Coalescence', xy=(3.82,max(new_means+std)-0.35),  size=35)\n",
    "    if run[:3]=='all':\n",
    "    \n",
    "        plt.annotate('All', xy=(0.02,1.02), xycoords='axes fraction', size=20)\n",
    "        plt.annotate('Early', xy=(1.5,0.97), size=20)\n",
    "        \n",
    "        plt.annotate('Late', xy=(1.9,0.97), size=20)\n",
    "        plt.annotate('Post Coalescence', xy=(2.75,0.97), size=20)\n",
    "    if run[:9]=='fg3_m1_10':\n",
    "        plt.axvline(x=9.17+0.5, color='black', ls='--')\n",
    "        plt.axvline(x=9.17, color='black', ls='--')\n",
    "        plt.axvline(x=7.8, color='black', ls='--')\n",
    "        plt.annotate('q0.1_fg0.3_BT0.2', xy=(0.07,max(new_means+std)-0.35), size=35)\n",
    "        plt.annotate('Early', xy=(6,max(new_means+std)-0.35), size=35)\n",
    "        \n",
    "        plt.annotate('Late', xy=(8,max(new_means+std)-0.35), size=35)\n",
    "        plt.annotate('Post Coalescence', xy=(9.25,max(new_means+std)-0.35), size=35)\n",
    "    #plt.ylim([-1,1])\n",
    "    \n",
    "    frame1 = plt.gca()\n",
    "    plt.tick_params(axis='both', which='major', labelsize=30)\n",
    "    \n",
    "    #frame1.axes.yaxis.set_ticklabels([])\n",
    "    plt.axhline(y=mean_non, color='black')\n",
    "    plt.xlabel(r'Merger Timeline [Gyr]', size=35)\n",
    "    plt.ylabel(r'LD1', size=35)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Mountain_plot_kin_'+str(run)+'.pdf')\n",
    "    \n",
    "    \n",
    "    import pandas\n",
    "    import seaborn\n",
    "    continue\n",
    "    '''lf = pandas.melt(df, value_vars=['Delta PA', 'v_asym', 's_asym', 'resids'], id_vars='class label')\n",
    "    print(lf.sample(7))'''\n",
    "    seasons = [0,1]\n",
    "\n",
    "    fg = (\n",
    "        pandas.melt(df, value_vars=['Delta PA', 's_asym', 'resids','lambda_r','epsilon','A_2'], id_vars='class label')\n",
    "            .pipe(\n",
    "                (seaborn.factorplot, 'data'), # (<fxn>, <dataframe var>)\n",
    "                kind='box',                   # type of plot we want\n",
    "                x='class label', x_order=seasons,  # x-values of the plots\n",
    "                y='value', palette='RdBu_r',  # y-values and colors\n",
    "                col='variable', col_wrap=3,   # 'A-D' in columns, wrap at 2nd col\n",
    "                sharey=False ,                 # tailor y-axes for each group\n",
    "                notch=False, width=0.75,       # kwargs passed to boxplot\n",
    "            )\n",
    "    )\n",
    "\n",
    "    plt.savefig('Clean_box_'+str(run)+'.pdf')\n",
    "    \n",
    "    '''Now, make hte k-fold cross validation plot'''\n",
    "    num_comps_list.append(num_comps)\n",
    "    missclass_list_1.append(missclass)\n",
    "    missclass_list_1_e.append(missclass_e)\n",
    "    min_comps_list.append(min_comps)\n",
    "    min_A_list.append(min_A)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        if argrelextrema(np.array(missclass), np.less)[0]:\n",
    "            \n",
    "            min_A=missclass[argrelextrema(np.array(missclass), np.less)[0][0]]\n",
    "            min_comps=num_comps[argrelextrema(np.array(missclass), np.less)[0][0]]\n",
    "    \n",
    "        else:\n",
    "            \n",
    "            min_A=missclass[locate_min(missclass)[1][0]]\n",
    "            min_comps=num_comps[locate_min(missclass)[1][0]]\n",
    "    except ValueError:\n",
    "        \n",
    "        min_A=missclass[argrelextrema(np.array(missclass), np.less)[0][0]]\n",
    "        min_comps=num_comps[argrelextrema(np.array(missclass), np.less)[0][0]]\n",
    "\n",
    "    \n",
    "    \n",
    "plt.clf()\n",
    "for j in range(len(min_comps_list)-2):\n",
    "    plt.plot(num_comps_list[j], missclass_list_1[j], color=colors[j], label=names[j])\n",
    "    plt.errorbar(num_comps_list[j], missclass_list_1[j],yerr=missclass_list_1_e[j], color=colors[j])\n",
    "    plt.scatter(min_comps_list[j], min_A_list[j], marker='x', color='black', zorder=100)\n",
    "plt.xlabel('Number of Predictors', size=25)\n",
    "plt.ylabel('Cross-Validation Error', size=25)\n",
    "#plt.ylim([0,8])\n",
    "plt.legend(fontsize=20)\n",
    "#yint = range(min(missclass_list), math.ceil(max(missclass_list))+1)\n",
    "\n",
    "#plt.yticks(yint)\n",
    "#plt.ylim([min(missclass_list), math.ceil(max(missclass_list))])\n",
    "\n",
    "#xint = range(min(num_comps), math.ceil(max(num_comps))+1)\n",
    "#plt.xticks(xint)\n",
    "plt.xlim([0,15])\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.savefig('CV_variable_selection_kin_interaction.pdf')\n",
    "\n",
    " \n",
    "    \n",
    "\n",
    "print('finished')\n",
    "print('myr_total', myr_total)\n",
    "print('LDA_time',LDA_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 6) [ 0.38 -0.7   1.74 -1.94  0.44  0.53]\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'> (7, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beckynevin/anaconda/lib/python3.6/site-packages/matplotlib/pyplot.py:537: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np   \n",
    "matrix = np.array([[0.38,-0.7,1.74,-1.94,0.44,0.53],\n",
    "                   [1.01,1.08,-0.43,0,0,0.28],\n",
    "                   [0,1.69,0,0,0,0.69],\n",
    "          [0,3.02,1.01,-2.22,0,1.48],#& \\textbf{2.09 $\\pm$ 0.65} & \\textbf{-5.24 $\\pm$ 1.38} & -- & \\textbf{13.34 $\\pm$ 0.78} & -- & -- & \\textbf{10.72 $\\pm$ 1.22} & \\textbf{-0.75 $\\pm$ 0.07}\\\\\n",
    "          [0,0.86,0.53,-2.55,0.71,0],\n",
    "          [0,1.79,-1.28,-0.95,0,0],\n",
    "                  [1.25,0,0,0,0,0]])\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "print(matrix.shape,matrix[0])\n",
    "matrix_row_0=matrix[0]/max(abs(matrix[0]))\n",
    "matrix_row_1=matrix[1]/max(abs(matrix[1]))\n",
    "matrix_row_2=matrix[2]/max(abs(matrix[2]))\n",
    "matrix_row_3=matrix[3]/max(abs(matrix[3]))\n",
    "matrix_row_4=matrix[4]/max(abs(matrix[4]))\n",
    "matrix_row_5=matrix[5]/max(abs(matrix[5]))\n",
    "matrix_row_6=matrix[6]/max(abs(matrix[6]))\n",
    "\n",
    "\n",
    "matrix_new=[matrix_row_0,matrix_row_1,matrix_row_2,matrix_row_3,matrix_row_4,matrix_row_5,matrix_row_6]\n",
    "\n",
    "matrix_new=np.array(matrix_new)\n",
    "print(type(matrix),type(matrix_new), np.shape(matrix_new))\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "fig=plt.figure(figsize=(25,15))\n",
    "ax=fig.add_subplot(111)\n",
    "my_cmap = ListedColormap(sns.color_palette(\"BuGn\").as_hex())\n",
    "\n",
    "\n",
    "#sns.diverging_palette(10, 220, sep=80, n=7)\n",
    "sns.set_style('dark')\n",
    "im=ax.imshow(abs(matrix_new), cmap=my_cmap)\n",
    "tick_marks_x = np.arange(np.shape(matrix_new)[1])\n",
    "tick_marks_y = np.arange(np.shape(matrix_new)[0])\n",
    "\n",
    "target_names_x=[r'$\\Delta$PA',r'$\\sigma_{\\mathrm{asym}}$',r'resids',r'$\\lambda_{R_e}$',r'$\\epsilon$',r'$A_2$']\n",
    "target_names_y=['All Major','All Minor','q0.5_fg0.3', 'q0.333_fg0.3','q0.333_fg0.1','q0.2_fg0.3_BT0.2','q0.1_fg0.3_BT0.2']\n",
    "plt.xticks(tick_marks_x, target_names_x, size=35)#, rotation=45)\n",
    "plt.yticks(tick_marks_y, target_names_y, size=35)\n",
    "cbar=plt.colorbar(im)#, label='Normalized Predictor Power')\n",
    "cbar.ax.tick_params(labelsize=25) \n",
    "\n",
    "\n",
    "fmt = '.2f' \n",
    "thresh = 0.65\n",
    "    \n",
    "for i, j in itertools.product(range(matrix_new.shape[0]), range(matrix_new.shape[1])):\n",
    "    if matrix_new[i, j]==0.00:\n",
    "        continue\n",
    "    else:\n",
    "        plt.text(j, i, format(matrix_new[i, j], fmt),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if matrix_new[i, j] > thresh else \"black\", size=25)\n",
    "plt.tight_layout()\n",
    "#plt.show()\n",
    "plt.savefig('color_predictors_kin.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mosthighlycorrelated(mydataframe, numtoreport):\n",
    "    # find the correlations\n",
    "    cormatrix = mydataframe.corr()\n",
    "    # set the correlations on the diagonal or lower triangle to zero,\n",
    "    # so they will not be reported as the highest ones:\n",
    "    cormatrix *= np.tri(*cormatrix.values.shape, k=-1).T\n",
    "    # find the top n correlations\n",
    "    cormatrix = cormatrix.stack()\n",
    "    cormatrix = cormatrix.reindex(cormatrix.abs().sort_values(ascending=False).index).reset_index()\n",
    "    # assign human-friendly names\n",
    "    cormatrix.columns = [\"FirstVariable\", \"SecondVariable\", \"Correlation\"]\n",
    "    return cormatrix.head(numtoreport)\n",
    "\n",
    "def hinton(matrix, max_weight=None, ax=None):\n",
    "    \"\"\"Draw Hinton diagram for visualizing a weight matrix.\"\"\"\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "\n",
    "    if not max_weight:\n",
    "        max_weight = 2**np.ceil(np.log(np.abs(matrix).max())/np.log(2))\n",
    "\n",
    "    ax.patch.set_facecolor('lightgray')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    for (x, y), w in np.ndenumerate(matrix):\n",
    "        color = 'red' if w > 0 else 'blue'\n",
    "        size = np.sqrt(np.abs(w))\n",
    "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
    "                             facecolor=color, edgecolor=color)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    nticks = matrix.shape[0]\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.set_xticks(range(nticks))\n",
    "    labels=[r'$\\Delta PA$',r'$v_{\\mathrm{asym}}$',r'$\\sigma_{\\mathrm{asym}}$',r'resids',r'$\\lambda_R$',\n",
    "            r'$\\epsilon$',r'$A$',r'$A_2$']\n",
    "    ax.set_xticklabels(labels, rotation=90)\n",
    "    ax.set_yticks(range(nticks))\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.grid(False)\n",
    "\n",
    "    ax.autoscale_view()\n",
    "    ax.invert_yaxis()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
